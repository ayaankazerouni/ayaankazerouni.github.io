<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta http-equiv="x-ua-compatible" content="ie=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<title>Home - Ayaan M. Kazerouni</title>

	<link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/assets/academicons/css/academicons.min.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.1/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
	<link rel="canonical" href="ayaankazerouni.github.io/">
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300,700,800,600' rel='stylesheet' type='text/css'>
	<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
	<link href='https://fonts.googleapis.com/css?family=Muli:400,300' rel='stylesheet' type='text/css'>
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />

</head>


<body>
	<aside>
    <nav>
        <ul>
        
            <li><a href="/#" class='nav-link'>Home</a></li>
        
            <li><a href="/#projects" class='nav-link'>Projects</a></li>
        
            <li><a href="/#publications" class='nav-link'>Publications</a></li>
        
            <li><a href="/#teaching" class='nav-link'>Teaching</a></li>
        
            <li><a href="/#other-activities" class='nav-link'>Other</a></li>
        
        </ul>
    </nav>
</aside>


<header>
  <img class='img-self' src="/assets/images/self.jpg" alt='A picture of Ayaan M. Kazerouni' />
	<div class='info'>
    <h1><a href="/">Ayaan M. Kazerouni</a></h1>
    <a href="mailto:ayaan@vt.edu" class='address'>ayaan@vt.edu</a><br>
    <strong>Asst. Professor of Computer Science</strong><br>
    <a href="https://csc.calpoly.edu/"><strong>Cal Poly, San Luis Obispo</strong></a>
    <div class='contact'>
      <a href='https://github.com/ayaankazerouni'>
        <i class='fab fa-github' aria-label="GitHub", title="GitHub"></i>
        <span class='visible-hidden'>GitHub profile</span>
      </a>
      <a href='https://www.linkedin.com/in/ayaankazerouni'>
        <i class='fab fa-linkedin' aria-label="LinkedIn" title="LinkedIn"></i>
        <span class='visible-hidden'>LinkedIn profile</span>
      </a>
      <a href='https://twitter.com/ayaankazerouni'>
        <i class='fab fa-twitter-square' aria-label="Twitter" title="Twitter"></i>
        <span class='visible-hidden'>Twitter profile</span>
      </a>
      <a href='https://scholar.google.com/citations?user=jGSnCrUAAAAJ&hl=en'>
        <i class='ai ai-google-scholar-square' aria-label="Google Scholar" title="Google Scholar"></i>
        <span class='visible-hidden'>Google Scholar profile</span>
      </a>
      <a href='https://orcid.org/0000-0002-6574-1278'>
        <i class='ai ai-orcid-square' aria-label="OrcID" title="OrcID"></i>
        <span class='visible-hidden'>OrcID profile</span>
      </a>
    </div>
</header>

	<main>
		<article>
			<h2>Biography</h2>
<div class='content-box'>
<div class='content'>
<p>I obtained my PhD in Computer Science at <a href="https://cs.vt.edu">Virginia Tech</a> in 2020, under the co-advisement of <a href="http://people.cs.vt.edu/~shaffer/">Cliff Shaffer</a> and <a href="https://scholar.google.com/citations?user=5pV6DQ4AAAAJ&amp;hl=en&amp;oi=sra">Steve Edwards</a>.</p>
<p>I am broadly interested in <strong>computing education</strong> and <strong>software engineering</strong>.
I work on methods to assess elements of the personal software process like incremental development, test writing, and test quality.
The end goal is to provide student software developers with formative feedback about their development habits as they work on projects.</p>
<p>Find out more about my current and past work <a href="/#projects">below</a>.</p>
<p>I'm an international student from Mumbai, India, and have been living and studying in the United States since 2011.</p>
<p>
<a href='/assets/cv.pdf' target='_blank'>My CV (May 2020)</a>
</p>
</div>
</div>
<h2>Education</h2>
<div class='content-box'>
<div class='content'>
<div class='degree'>PhD, Computer Science</div>
<div class='sub'>
Virginia Polytechnic Institute and State University,
May 2020
<br>
</div>
<details>
<summary>
<i style='padding-left: 1.5em;'>
<a href='https://vtechworks.lib.vt.edu/handle/10919/97723'>Measuring the Software Development Process to Enable Formative Feedback</a>
</i>
</summary>
<p>
<b>
Abstract:
</b>
Graduating CS students face well-documented difficulties upon entering the workforce, with reports of a gap between what they learn and what is expected of them in industry. Project management, software testing, and debugging have been repeatedly listed as common "knowledge deficiencies" among newly hired CS graduates. Similar difficulties manifest themselves on a smaller scale in upper-level CS courses, like the Data Structures and Algorithms course at Virginia Tech: students are required to develop large and complex projects over a three to four week lifecycle, and it is common to see close to a quarter of the students drop or fail the course, largely due to the difficult and time-consuming nature of the projects. My research is driven by the hypothesis that regular feedback about the software development process, delivered during development, will help ameliorate these difficulties. Assessment of software currently tends to focus on qualities like correctness, code coverage from test suites, and code style. Little attention or tooling has been developed for the assessment of the software development process. I use empirical software engineering methods like IDE-log analysis, software repository mining, and semi-structured interviews with students to identify effective and ineffective software practices to formulate. Using the results of these analyses, I have worked on assessing students' development in terms of time management, test writing, test quality, and other "self-checking" behaviours like running the program locally or submitting to an oracle of instructor-written test cases. The goal is to use this information to formulate formative feedback about the software development process. In addition to educators, this research is relevant to software engineering researchers and practitioners, since the results from these experiments are based on the work of upper-level students who grapple with issues of design and work-flow that are not far removed from those faced by professionals in industry.
</p>
</details>
<div class='degree'>BS, Computer Science</div>
<div class='sub'>University of West Georgia, April 2015</div>
</div>
</div>
<h2 id='projects'>Projects</h2>
<div class='blurb'>
<p>I am interested in <strong>software engineering education</strong>. This section describes the projects I am currently working on or have worked on in the past.</p>
<p>Relevant publications are shown for each project. See <a href="#publications">a complete list</a> below.</p>
</div>
<div class='content-box' id='dev-event-tracker'>
<h3 class='title'>
Enabling formative assessment of the software development process (2015 - present)
</h3>
<div class='content'>
<p>We instrumented Eclipse to collect high-resolution “click-stream” data as developers program.
Using these data, we quantify the development process in terms of time management, testing, and debugging.
The goal is to identify effective or ineffective practices, and formulate and deploy <em>timely</em> and <em>intelligent</em> feedback.</p>
<strong>On GitHub:</strong>
<div class='artifact-list'>
<div class='artifact'>
<a href='https://github.com/web-cat/eclipse-plugins-importer-exporter/tree/DevEventTrackerAddition'>
web-cat/eclipse-plugins-importer-exporter
</a>
<div class='sub'>
The Eclipse plugin, with the data collection functionality found on the
 <i>DevEventTracker</i> 
branch
</div>
</div>
<div class='artifact'>
<a href='https://github.com/ayaankazerouni/sensordata'>ayaankazerouni/sensordata</a>
(<a href='https://ayaankazerouni.github.io/sensordata'>Documentation</a>)
<div class='sub'>Implementations of metrics proposed in the papers below, and various data management utilities</div>
</div>
<div class='artifact'>
<a href='https://github.com/ayaankazerouni/incremental-testing'>ayaankazerouni/incremental-testing</a>
<div class='sub'>
Repository mining, static analysis, and measurements of incremental testing behaviour
</div>
</div>
</div>
<strong>Selected papers:</strong>
<div class='artifact-list'>
<details class='artifact'>
<summary>
Assessing Incremental Testing Practices and Their Impact on Project Outcomes
<b>
(SIGCSE '19)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>, C. A. Shaffer, S. H. Edwards, F. Servant
<br>
<a class='artifact-link' href='assets/publications/assessing-incremental-testing.pdf'>pdf</a>
<a class='artifact-link' href='assets/slides/assessing-incremental-testing-slides.pdf'>slides</a>
<a class='artifact-link' href='https://doi.org/10.1145/3287324.3287366'>link</a>
<a class='artifact-link' href='https://github.com/ayaankazerouni/incremental-testing'>code</a>
<div class='award artifact-link'>
2nd Best Research Paper
</div>
</div>
</summary>
<p>
<b>Abstract:</b> Software testing is an important aspect of the development process, one that has proven to be a challenge to formally introduce into the typical undergraduate CS curriculum. Unfortunately, existing assessment of testing in student software projects tends to focus on evaluation of metrics like code coverage over the finished software product, thus eliminating the possibility of giving students early feedback as they work on the project. Furthermore, assessing and teaching the process of writing and executing software tests is also important, as shown by the multiple variants proposed and disseminated by the software engineering community, e.g., test-driven development (TDD) or incremental test-last (ITL). We present a family of novel metrics for assessment of testing practices for increments of software development work, thus allowing early feedback before the software project is finished. Our metrics measure the balance and sequence of effort spent writing software tests in a work increment. We performed an empirical study using our metrics to evaluate the test-writing practices of 157 advanced undergraduate students, and their relationships with project outcomes over multiple projects for a whole semester. We found that projects where more testing effort was spent per work session tended to be more semantically correct and have higher code coverage. The percentage of method-specific testing effort spent before production code did not contribute to semantic correctness, and had a negative relationship with code coverage. These novel metrics will enable educators to give students early, incremental feedback about their testing practices as they work on their software projects.
</p>
</details>
<details class='artifact'>
<summary>
Quantifying Incremental Development Practices and Their Relationship to Procrastination
<b>
(ICER '17)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>, S. H. Edwards, C. A. Shaffer
<br>
<a class='artifact-link' href='assets/publications/quantifying-incremental-development-procrastination.pdf'>pdf</a>
<a class='artifact-link' href='https://doi.org/10.1145/3105726.3106180'>link</a>
<a class='artifact-link' href='https://github.com/ayaankazerouni/sensordata'>code</a>
<a class='artifact-link' href='https://medium.com/@ayaankazerouni/developing-procrastination-feedback-for-student-software-developers-1652de60db7f'>blog</a>
</div>
</summary>
<p>
<b>Abstract:</b> We present quantitative analyses performed on character-level program edit and execution data, collected in a junior-level data structures and algorithms course. The goal of this research is to determine whether proposed measures of student behaviors such as incremental development and procrastination during their program development process are significantly related to the correctness of final solutions, the time when work is completed, or the total time spent working on a solution. A dataset of 6.3 million fine-grained events collected from each student's local Eclipse environment is analyzed, including the edits made and events such as running the program or executing software tests. We examine four primary metrics proposed as part of previous work, and also examine variants and refinements that may be more effective. We quantify behaviors such as working early and often, frequency of program and test executions, and incremental writing of software tests. Projects where the author had an earlier mean time of edits were more likely to submit their projects earlier and to earn higher scores for correctness. Similarly earlier median time of edits to software tests was also associated with higher correctness scores. No significant relationships were found with incremental test writing or incremental checking of work using either interactive program launches or running of software tests, contrary to expectations. A preliminary prediction model with 69% accuracy suggests that the underlying metrics may support early prediction of student success on projects. Such metrics also can be used to give targeted feedback to help students improve their development practices.
</p>
</details>
</div>
</div>
</div>
<div class='content-box' id='code-workout'>
<h3 class='title'>
<a href='https://codeworkout.cs.vt.edu'></a>
CodeWorkout - Drill-and-practice programming exercises for novices (2016 - present)
</h3>
<div class='content'>
<p>CodeWorkout is an online system for people learning a programming language for the first time.
It is a free, open-source solution for practicing small programming problems that is currently in use in several courses at Virginia Tech and other universities.</p>
<strong>On GitHub:</strong>
<div class='artifact-list'>
<div class='artifact'>
<a href='https://github.com/web-cat/code-workout'>
web-cat/code-workout
</a>
</div>
</div>
<strong>Selected papers:</strong>
<div class='artifact-list'>
<details class='artifact'>
<summary>
The Relationship Between Practicing Short Programming Exercises and Exam Performance
<b>
(CompEd '19)
</b>
<div class='sub'>
S. H. Edwards, K. P. Murali, <b>A. M. Kazerouni</b>
<br>
<a class='artifact-link' href='assets/publications/codeworkout-comped-2019.pdf'>pdf</a>
<a class='artifact-link' href='https://doi.org/10.1145/3300115.3309525'>link</a>
</div>
</summary>
<p>
<b>Abstract:</b> Learning to program can be challenging. Many instructors use drill-and-practice strategies to help students develop basic programming techniques and improve their confidence. Online systems that provide short programming exercises with immediate, automated feedback are seeing more frequent use in this regard. However, the relationship between practicing with short programming exercises and performance on larger programming assignments or exams are unclear. This paper describes an evaluation of short programming questions in the context of a CS1 course where they were used on both homework assignments, for practice and learning, and on exams, for assessing individual performance. The open-source drill-and-practice system used here provides for full feedback during practice exercises. During exams, it allows limiting feedback to compiler errors and to a very small number of example inputs shown in the question, instead of the more complete feedback received during practice. Using data collected from 200 students in a CS1 course, we examine the relationship between voluntary practice on short exercises and subsequent performance on exams, while using an early exam as a control for individual differences including ability level. Results indicate that, after controlling for ability, voluntary practice does contribute to improved performance on exams, but that motivation to improve may also be important.
</p>
</details>
</div>
</div>
</div>
<h2 id='publications'>Publications</h2>
<div class='content-box'>
<h3 class='title'>
Conference papers
</h3>
<div class='content'>
<div class='artifact-list'>
<details class='artifact'>
<summary>
ProgSnap2: A Flexible Format for Programming Process Data
<b>
(ITiCSE '20)
</b>
<div class='sub'>
T. Price, D. Hovemeyer, K. Rivers, A. C. Bart, G. Gao, <b>A. M. Kazerouni</b>, B. Becker, A. Petersen, L. Gusukuma, S. H. Edwards, D. Babcock
<br>
<a class='artifact-link' href='assets/publications/iticse20-progsnap2.pdf'>pdf</a>
<a class='artifact-link' href='https://doi.org/10.1145/3341525.3387373'>link</a>
</div>
</summary>
<p>
<b>Abstract:</b> In this paper, we introduce ProgSnap2, a standardized format for logging programming process data. ProgSnap2 is a tool for computing education researchers, with the goal of enabling collaboration by helping them to collect and share data, analysis code, and data-driven tools to support students. We give an overview of the format, including how events, event attributes, metadata, code snapshots and external resources are represented. We also present a case study to evaluate how ProgSnap2 can facilitate collaborative research. We investigated three metrics designed to quantify students' difficulty with compiler errors - the Error Quotient, Repeated Error Density and Watwin score - and compared their distributions and ability to predict students' performance. We analyzed five different ProgSnap2 datasets, spanning a variety of contexts and programming languages. We found that each error metric is mildly predictive of students' performance. We reflect on how the common data format allowed us to more easily investigate our research questions.
</p>
</details>
<details class='artifact'>
<summary>
Testing Regex Generalizability And Its Implications: A Large-Scale Many-Language Measurement Study
<b>
(ASE '19)
</b>
<div class='sub'>
J. C. Davis, D. Moyer, <b>A. M. Kazerouni</b>, D. Lee
<br>
<a class='artifact-link' href='assets/publications/regex-generalizability-2019.pdf'>pdf</a>
<a class='artifact-link' href='https://doi.org/10.1109/ASE.2019.00048'>link</a>
</div>
</summary>
<p>
<b>Abstract:</b> The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and findings to date are typically generalized from regexes written in only 1-2 programming languages. This is an incomplete foundation. Generalizing existing research depends on validating two hypotheses: (1) Various regex extraction methodologies yield similar results, and (2) Regex characteristics are similar across programming languages. To test these hypotheses, we defined eight regex metrics to capture the dimensions of regex representation, string language diversity, and worst-case match complexity. We report that the two competing regex extraction methodologies yield comparable corpuses, suggesting that simpler regex extraction techniques will still yield sound corpuses. But in comparing regexes across programming languages, we found significant differences in some characteristics by programming language. Our findings have bearing on future empirical methodology, as the programming language should be considered, and generalizability will not be assured. Our measurements on a corpus of 537,806 regexes can guide data-driven designs of a new generation of regex tools and regex engines.
</p>
</details>
<details class='artifact'>
<summary>
The Relationship Between Practicing Short Programming Exercises and Exam Performance
<b>
(CompEd '19)
</b>
<div class='sub'>
S. H. Edwards, K. P. Murali, <b>A. M. Kazerouni</b>
<br>
<a class='artifact-link' href='assets/publications/codeworkout-comped-2019.pdf'>pdf</a>
<a class='artifact-link' href='https://doi.org/10.1145/3300115.3309525'>link</a>
</div>
</summary>
<p>
<b>Abstract:</b> Learning to program can be challenging. Many instructors use drill-and-practice strategies to help students develop basic programming techniques and improve their confidence. Online systems that provide short programming exercises with immediate, automated feedback are seeing more frequent use in this regard. However, the relationship between practicing with short programming exercises and performance on larger programming assignments or exams are unclear. This paper describes an evaluation of short programming questions in the context of a CS1 course where they were used on both homework assignments, for practice and learning, and on exams, for assessing individual performance. The open-source drill-and-practice system used here provides for full feedback during practice exercises. During exams, it allows limiting feedback to compiler errors and to a very small number of example inputs shown in the question, instead of the more complete feedback received during practice. Using data collected from 200 students in a CS1 course, we examine the relationship between voluntary practice on short exercises and subsequent performance on exams, while using an early exam as a control for individual differences including ability level. Results indicate that, after controlling for ability, voluntary practice does contribute to improved performance on exams, but that motivation to improve may also be important.
</p>
</details>
<details class='artifact'>
<summary>
Assessing Incremental Testing Practices and Their Impact on Project Outcomes
<b>
(SIGCSE '19)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>, C. A. Shaffer, S. H. Edwards, F. Servant
<br>
<a class='artifact-link' href='assets/publications/assessing-incremental-testing.pdf'>pdf</a>
<a class='artifact-link' href='assets/slides/assessing-incremental-testing-slides.pdf'>slides</a>
<a class='artifact-link' href='https://doi.org/10.1145/3287324.3287366'>link</a>
<a class='artifact-link' href='https://github.com/ayaankazerouni/incremental-testing'>code</a>
<div class='award artifact-link'>
2nd Best Research Paper
</div>
</div>
</summary>
<p>
<b>Abstract:</b> Software testing is an important aspect of the development process, one that has proven to be a challenge to formally introduce into the typical undergraduate CS curriculum. Unfortunately, existing assessment of testing in student software projects tends to focus on evaluation of metrics like code coverage over the finished software product, thus eliminating the possibility of giving students early feedback as they work on the project. Furthermore, assessing and teaching the process of writing and executing software tests is also important, as shown by the multiple variants proposed and disseminated by the software engineering community, e.g., test-driven development (TDD) or incremental test-last (ITL). We present a family of novel metrics for assessment of testing practices for increments of software development work, thus allowing early feedback before the software project is finished. Our metrics measure the balance and sequence of effort spent writing software tests in a work increment. We performed an empirical study using our metrics to evaluate the test-writing practices of 157 advanced undergraduate students, and their relationships with project outcomes over multiple projects for a whole semester. We found that projects where more testing effort was spent per work session tended to be more semantically correct and have higher code coverage. The percentage of method-specific testing effort spent before production code did not contribute to semantic correctness, and had a negative relationship with code coverage. These novel metrics will enable educators to give students early, incremental feedback about their testing practices as they work on their software projects.
</p>
</details>
<details class='artifact'>
<summary>
Quantifying Incremental Development Practices and Their Relationship to Procrastination
<b>
(ICER '17)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>, S. H. Edwards, C. A. Shaffer
<br>
<a class='artifact-link' href='assets/publications/quantifying-incremental-development-procrastination.pdf'>pdf</a>
<a class='artifact-link' href='https://doi.org/10.1145/3105726.3106180'>link</a>
<a class='artifact-link' href='https://github.com/ayaankazerouni/sensordata'>code</a>
<a class='artifact-link' href='https://medium.com/@ayaankazerouni/developing-procrastination-feedback-for-student-software-developers-1652de60db7f'>blog</a>
</div>
</summary>
<p>
<b>Abstract:</b> We present quantitative analyses performed on character-level program edit and execution data, collected in a junior-level data structures and algorithms course. The goal of this research is to determine whether proposed measures of student behaviors such as incremental development and procrastination during their program development process are significantly related to the correctness of final solutions, the time when work is completed, or the total time spent working on a solution. A dataset of 6.3 million fine-grained events collected from each student's local Eclipse environment is analyzed, including the edits made and events such as running the program or executing software tests. We examine four primary metrics proposed as part of previous work, and also examine variants and refinements that may be more effective. We quantify behaviors such as working early and often, frequency of program and test executions, and incremental writing of software tests. Projects where the author had an earlier mean time of edits were more likely to submit their projects earlier and to earn higher scores for correctness. Similarly earlier median time of edits to software tests was also associated with higher correctness scores. No significant relationships were found with incremental test writing or incremental checking of work using either interactive program launches or running of software tests, contrary to expectations. A preliminary prediction model with 69% accuracy suggests that the underlying metrics may support early prediction of student success on projects. Such metrics also can be used to give targeted feedback to help students improve their development practices.
</p>
</details>
<details class='artifact'>
<summary>
DevEventTracker: Tracking development events to assess incremental development and procrastination
<b>
(ITiCSE '17)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>, S. H. Edwards, T. S. Hall, C. A. Shaffer
<br>
<a class='artifact-link' href='assets/publications/deveventtracker.pdf'>pdf</a>
<a class='artifact-link' href='https://doi.org/10.1145/3059009.3059050'>link</a>
<a class='artifact-link' href='https://github.com/ayaankazerouni/sensordata'>code</a>
<a class='artifact-link' href='https://medium.com/@ayaankazerouni/developing-procrastination-feedback-for-student-software-developers-1652de60db7f'>blog</a>
</div>
</summary>
<p>
<b>Abstract:</b> Good project management practices are hard to teach, and hard for novices to learn. Procrastination and bad project management practice occur frequently, and may interfere with successfully completing major programming projects in mid-level programming courses. Students often see these as abstract concepts that do not need to be actively applied in practice. Changing student behavior requires changing how this material is taught, and more importantly, changing how learning and practice are assessed. To provide proper assessment, we need to collect detailed data about how each student conducts their project development as they work on solutions. We present DevEventTracker, a system that continuously collects data from the Eclipse IDE as students program, giving us in-depth insight into students' programming habits. We report on data collected using DevEventTracker over the course of four programming projects involving 370 students in five sections of a Data Structures and Algorithms course over two semesters. These data support a new measure for how well students apply "incremental development" practices. We present a detailed description of the system, our methodology, and an initial evaluation of our ability to accurately assess incremental development on the part of the students. The goal is to help students improve their programming habits, with an emphasis on incremental development and time management.
</p>
</details>
</div>
</div>
</div>
<div class='content-box'>
<h3 class='title'>Abstracts and posters</h3>
<div class='content'>
<div class='artifact-list'>
<details class='artifact'>
<summary>
Toward Continuous Assessment of the Programming Process &mdash; Doctoral Consortium
<b>
(ICER '19)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>
<br>
<a href='assets/publications/icer-dc.pdf'>pdf</a>
<a href='https://doi.org/10.1145/3291279.3339429'>link</a>
</div>
</summary>
<p>
<b>Abstract:</b> Assessment of software tends to focus on postmortem evaluation of metrics like correctness, mergeability, and code coverage. This is evidenced in the current practices of continuous integration and deployment that focus on software's ability to pass unit tests before it can be merged into a deployment pipeline. However, little attention or tooling is given to the assessment of the software development process itself. Good process becomes both more challenging and more critical as software complexity increases. Real-time evaluation and feedback about a student's software development skills, such as incremental development, testing, and time management, could greatly increase productivity and improve the ability to write tested and correct code. In my research, I develop models to quantify a student's programming process in terms of these metrics. By measuring the programming process, I can empirically evaluate its adherence to known best practices in software engineering. With the ability to characterize this, I can build tools to provide them with intelligent and timely feedback when they are in danger of straying from those practices. In the long term, I hope to contribute to the standardization and adoption of continuous software assessment techniques that include not only the final product, but also the process undertaken to produce it.
</p>
</details>
<details class='artifact'>
<summary>
Student Debugging Practices and Their Relationships with Project Outcomes &mdash; Poster
<b>
(SIGCSE '19)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>, R. S. Mansur, S. H. Edwards, C. A. Shaffer
<br>
<a href='assets/documents/sigcse2019-debugging-poster.pdf'>pdf</a>
<a href='https://doi.org/10.1145/3287324.3293794'>link</a>
</div>
</summary>
<p>
<b>Abstract:</b> Debugging is an important part of the software development process, studied by both the CS education and software engineering communities. Most prior work has focused either on novice or professional programmers. Intermediate-to-advanced students (such as those enrolled in post-CS2 Data Structures courses) who are working on large and complex projects have largely been ignored. We present results from an empirical observational study that examined junior-level undergraduate students' debugging practices on relatively large (4-week lifecycle) projects, using IDE clickstream data collected by a custom Eclipse plugin. Specifically, we hypothesize that there are differing debugging behaviors exhibited, and that differing behaviors lead to differing project out-comes. For example, how often do students use the symbolic debugger available in modern IDEs, versus how often do they use diagnostic print statements, or both? What triggers a debugging session? What follows a debugging session? Does it matter when in the project lifecycle that debugging takes place? We have a number of interesting preliminary results. When using the debugger, there was a negative relationship between step-over and step-into actions versus final course grades, indicating that when students "spin their wheels" while debugging, they tend to perform more poorly. Students also tend to perform better on the project when debugging takes place earlier in the overall project life-cycle. We developed an algorithm to identify diagnostic print statements in the students' projects. We found that over 90% used at least one diagnostic print statement, and about 75% used the symbolic debugger, at least once in any given project.
</p>
</details>
<details class='artifact'>
<summary>
Toward Continuous Assessment of the Programming Process &mdash; Student Research Competition
<b>
(SIGCSE '18)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>
<br>
<a href='assets/documents/sigcse-src-2018-poster.pdf'>pdf</a>
<a href='assets/slides/sigcse2018-src-slides.pdf'>slides</a>
<a href='https://doi.org/10.1145/3159450.3162337'>link</a>
<div class='award'>
1st Place
</div>
</div>
</summary>
<p>
<b>Abstract:</b> Assessment of software tends to focus on postmortem evaluation of metrics like correctness, mergeability, and code coverage. This is evidenced in the current practices of continuous integration and deployment that focus on software's ability to pass unit tests before it can be merged into a deployment pipeline. However, little attention or tooling is given to the assessment of the software development process itself. Good process becomes both more challenging and more critical as software complexity increases. Real-time evaluation and feedback about a software developer's skills, such as incremental development, testing, and time management, could greatly increase productivity and improve the ability to write tested andcorrect code. My work focuses on the collection and analysis of fine-grained programming process data to help quantitatively model the programming process in terms of these metrics. I report on my research problem, presenting past work involving the collection and analysis of IDE event data from junior level students working on large and complex projects. The goal is to quantify the programming process in terms of incremental development and procrastination. I also present a long-term vision for my research and present work planned in the short term as a step toward that vision.
</p>
</details>
</div>
</div>
</div>
<h2 id='teaching'>Teaching</h2>
<div class='content-box'>
<div class='content'>
<ul>
<li>In Summer 2018 and Summer 2019, I was instructor of record for <strong>CS 3114: Data Structures &amp; Algorithms</strong>, an upper-level course required for CS majors.</li>
<li>In Fall 2015 and Spring 2016, I was a teaching assistant for <strong>CS 3114</strong>. This is also when I began my <a href="#dev-event-tracker">research</a> into how to
better help students successfully complete large and complex programming projects.</li>
<li>From 2012 to 2015, I was an undergraduate TA for <strong>Introduction to CS, CS I, CS II, Intro to Web Development</strong>
and <strong>Software Engineering I</strong> at the University of West Georgia.</li>
</ul>
</div>
</div>
<h2 id='other-activities'>Other Activities</h2>
<div class='content-box'>
<h3 class='title'>
Hobbies
</h3>
<div class='content'>
<h4>Music</h4>
<p>I like playing and listening to folk music, but there are gems in every genre.</p>
<p>I played guitar in a band of fellow PhD students at Virginia Tech.
We called ourselves <a href="https://www.facebook.com/pandabagband/">Panda Bag</a>, and mostly played at campus events organized by the grad school.
Check us out on <a href="https://www.youtube.com/watch?v=BO_onwvDT60&amp;list=PLF9ahQmIu82kd3s8ecrgArmxrUPwYiEXU&amp;index=6">YouTube</a>!</p>
<h4>Books and TV</h4>
<p>I enjoy reading books and watching TV shows.
Fantasy and historical fiction are my favourite genres, but I'm open to others.
Find me on <a href="https://www.goodreads.com/user/show/41197961-ayaan-kazerouni">Goodreads</a>!</p>
<h4>Sports</h4>
<p>I like hiking and racquetball.
I'm a huge fan of attempting a regular exercise schedule once every few weeks.</p>
</div>
</div>


		</article>
		<footer>
  <div class='get-in-touch'>Get in touch.<br></div>
  <div class='address'>ayaan [at] vt [dot] edu</div>
  <div class='address'>2000 Torgersen Hall, 620 Drillfield Drive</div>
  <div class='contact'>
    <a href='https://github.com/ayaankazerouni'>
      <i class='fab fa-github' aria-label="GitHub", title="GitHub"></i>
      <span class='visible-hidden'>GitHub profile</span>
    </a>
    <a href='https://www.linkedin.com/in/ayaankazerouni'>
      <i class='fab fa-linkedin' aria-label="LinkedIn" title="LinkedIn"></i>
      <span class='visible-hidden'>LinkedIn profile</span>
    </a>
    <a href='https://twitter.com/ayaankazerouni'>
      <i class='fab fa-twitter-square' aria-label="Twitter" title="Twitter"></i>
      <span class='visible-hidden'>Twitter profile</span>
    </a>
    <a href='https://scholar.google.com/citations?user=jGSnCrUAAAAJ&hl=en'>
      <i class='ai ai-google-scholar-square' aria-label="Google Scholar" title="Google Scholar"></i>
      <span class='visible-hidden'>Google Scholar profile</span>
    </a>
    <a href='https://orcid.org/0000-0002-6574-1278'>
      <i class='ai ai-orcid-square' aria-label="OrcID" title="OrcID"></i>
      <span class='visible-hidden'>OrcID profile</span>
    </a>
  </div>
</footer>

	</main>
</body>

</html>
