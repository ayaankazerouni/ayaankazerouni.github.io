<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://ayaankazerouni.org/posts/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ayaankazerouni.org/" rel="alternate" type="text/html" /><updated>2025-01-21T15:12:26-08:00</updated><id>https://ayaankazerouni.org/posts/feed.xml</id><title type="html">Ayaan M. Kazerouni</title><subtitle>Ayaan Kazerouni&apos;s personal website. Contains information about his research in computing education and software engineering, and  his hobbies and interests.
</subtitle><entry><title type="html">Challenges with learning end-user programming: A case study with Chemistry undergraduate students</title><link href="https://ayaankazerouni.org/posts/end-user-programming" rel="alternate" type="text/html" title="Challenges with learning end-user programming: A case study with Chemistry undergraduate students" /><published>2024-06-25T00:00:00-07:00</published><updated>2024-06-25T00:00:00-07:00</updated><id>https://ayaankazerouni.org/posts/end-user-programming</id><content type="html" xml:base="https://ayaankazerouni.org/posts/end-user-programming"><![CDATA[<p>This is an overview of the research paper <em><a href="/publications#jce2024end-user-programming">Recommendations for Improving End-User Programming Education: A Case Study with Undergraduate Chemistry Students</a></em>, published in the American Chemical Society (ACS) Journal of Chemical Education.</p>

<p>The work was conducted at Cal Poly and was led by MS student <a href="https://www.linkedin.com/in/wifuchs">Will Fuchs</a>.
Our co-authors were <a href="https://chemistry.calpoly.edu/content/faculty/ashley_mcdonald">Dr. Ashley McDonald</a> from the Chemistry &amp; Biochemistry department at Cal Poly and <a href="https://aakash.xyz">Dr. Aakash Gautam</a> from the CS department at the University of Pittsburgh.</p>

<p>This article should be of interest to educators in disciplines <strong>other than computer science or software engineering</strong> who are teaching their students programming.
For more details, see <a href="/publications#jce2024end-user-programming">the paper</a>.</p>

<h2 id="where-should-we-teach-chemists-to-program">Where should we teach Chemists to program?</h2>

<p>Professional software engineers are a minority of professionals who use programming in their work.
They are vastly outnumbered by professionals in other disciplines who use programming in their work (like chemists, climate scientists, or graphic designers).
These professionals are often the end-users of the software they create, and they are referred to as <em>end-user programmers</em>. 
Students preparing to enter these roles tend to learn <em>some</em> programming at the undergraduate level, through various channels, each with its own challenges.</p>

<p><strong>They could take the same intro programming course as computing majors</strong>, but this means they would be learning programming that is mostly disconnected from their discipline-specific context.
An additional challenge, at least at Cal Poly, is that computing departments tend to be over-enrolled and under-staffed.
If whole majors begin requiring their students to take our introductory CS course, we would quickly buckle under the enrolment pressure.
<!-- (Or, maybe everyone _should_ take it, just like they take Calculus, English, or History courses. But that's a separate conversation.) --></p>

<p><strong>They could take specially-designed contextualised computing courses</strong>, where they learn programming specifically geared toward their discipline.
For example, the Media Computation courses at Georgia Tech have been hugely successful at getting folks from different (demographic as well as professional) backgrounds interested in programming, and “CS + X” programs are sprouting up at universities all over the USA.
This is great, but difficult for a campus to operationalise for all its majors that would use programming.
If these courses were designed to cast a wide-enough net, we’d end up back at problem #1 above.</p>

<p><strong>They could learn programming within their discipline-specific courses</strong>.
An obvious challenge here is that most courses are already bursting at the seams with content, and the designers of those courses are often loath to let go of much of it to make room for new content.
I’m the same—when I design a course I could swear that every single topic is absolutely essential.
But the reality is that about 85% of the material is essential, and the rest is just, like, my opinion, man.</p>

<p>So, if we want to add programming to discipline-specific courses to prepare, say, chemists to use programming in their work, we need Chemistry professors who know how to program, and who care enough about its importance in their discipline to include it in their already-full curricula.</p>

<p><strong>Happily, that describes the Physical Chemistry instructors at Cal Poly SLO.</strong></p>

<p>Typically during their third year of undergraduate studies, students majoring in Chemistry or Biochemistry at Cal Poly take a 3-course sequence of Physical Chemistry courses, in which they also learn programming.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>
Including programming alongside thermodynamics, kinetics, and mechanics certainly does add to the instructional obligations of these courses, but over a decade with the curriculum has also revealed desirable gestalt effects—students not only learned <em>to</em> program; they also learned Chemistry <em>through</em> programming.
Many students go on to successfully apply their newfound computing skills to real problems in their research labs at Cal Poly and beyond.</p>

<p>However, many students also seem to intensely dislike the programming aspects of the courses: they find it challenging, are unmoved by arguments that it would benefit them professionally, and generally don’t “reach for computing” to help them solve Chemistry problems outside the classroom.
Most of them also self-select out of future computing experiences, opting out of an elective Computational Chemistry course following the 3-course Physical Chemistry sequence.</p>

<h2 id="understanding-students-challenges-with-learning-programming">Understanding students’ challenges with learning programming</h2>

<p>We (Chemists and Computer Scientists at Cal Poly) conducted two studies to learn about the challenges that the students faced while learning programming in these Chemistry courses.
In the first study, we used surveys to learn about students’ attitudes toward programming and their programming abilities.
In the second study, we conducted one-on-one interviews to gain deeper insight into their attitudes and abilities.</p>

<h3 id="preliminary-surveys">Preliminary surveys</h3>

<p>We used the <a href="https://dl.acm.org/doi/10.1145/3287324.3287369"><strong>attitudes toward computing survey</strong></a> to measure students’ self-reported <em>confidence</em> with and <em>enjoyment</em> of computing, their perception of computing as being <em>important and useful</em>, and the strength of their sense that they <em>belong</em> in computing.</p>

<p>Results were…not fun.
On average, the students reported negative attitudes toward computing: low confidence, low enjoyment, and low sense of belonging to a computing community.
They did perceive computing to be marginally useful.</p>

<figure class="wrap">
  <a href="/assets/posts/images/end-user-programming/attitudes.png" target="_blank">
    
    <img src="/assets/posts/images/end-user-programming/attitudes.png" alt="A column chart showing the average reported confidence with and enjoyment of computing, sense of belonging in computing, and sense that computing is important." width="100" />
    
  </a>

  <figcaption>Students' average responses to ATC questions. Questions were answered on a 1–5 scale from <i>Strongly disagree</i> to <i>Strongly agree</i></figcaption>
</figure>

<p>To measure students’ abilities with programming, we used the <a href="https://peer.asee.org/mcs1-a-matlab-programming-concept-inventory-for-assessing-first-year-engineering-courses"><strong>MCS1 concept inventory</strong></a>, a peer-reviewed assessment of MATLAB knowledge. (It’s a multiple-choice test.)</p>

<p>Students generally performed poorly on the assessment.
Part of this is attributed to the fact that it included some language elements that they had not seen before.
But there were also challenges on questions that “should” have been doable.</p>

<h3 id="interviews">Interviews</h3>

<p>Ok, so the students had negative attitudes toward computing, and struggled with MATLAB questions.
That’s kind of all we can say from numerical responses to Likert questions.
We can say nothing about what causes these negative attitudes, or what exactly their misunderstandings are that lead to their difficulties on the test.</p>

<p>Will, the MS student leading the work, conducted one-on-one interviews with 8 students enrolled in the Physical Chemistry courses.
He asked them questions from the MCS1 assessment, and instead of framing them as multiple-choice questions, he framed them as open-ended short-answer questions, and asked the students to “think out loud” while answering them.
He recorded the interviews (with the students’ consent), and together we analysed the transcripts.</p>

<p>Details are in <a href="/publications#jce2024end-user-programming">the paper</a>, but here are the highlights:</p>

<ul>
  <li>We used <a href="https://kar.kent.ac.uk/23997/1/TaxonomyFuller.pdf">Fuller et al.’s Matrix Taxonomy</a> to categorise the level at which students were operating in terms of their programming knowledge. Students were generally competent with code comprehension involving patterns they had seen before.</li>
  <li>This served them well when they saw similar-looking programs, but stymied them when they were faced with different representations of the same concepts. They lacked an abstract enough mental model of MATLAB’s syntax and semantics.</li>
  <li>Without having been taught explicit strategies to solve programming problems, students faced challenges with program planning. This caused difficulties with multi-step problems, which were discouraging and daunting.</li>
</ul>

<p>For example, consider the following MATLAB expression (a Boolean AND): <code class="language-plaintext highlighter-rouge">and(a, b)</code></p>

<p>The students had previously only seen Boolean expressions in the context of conditional branching using <code class="language-plaintext highlighter-rouge">if</code> statements.</p>

<p>So, students who were correctly able to trace code involving conditional branching (e.g., <code class="language-plaintext highlighter-rouge">if and(a, b)</code>) were then <em>un</em>able to trace code that included <code class="language-plaintext highlighter-rouge">c = and(true, and(true, false))</code></p>

<p>Lacking abstract understandings for <em>statements</em> and <em>expressions</em>, students combined the notions of <code class="language-plaintext highlighter-rouge">if</code> and <code class="language-plaintext highlighter-rouge">and</code> into a single construct based on superficial characteristics of programs they had seen before. Students faced similar difficulties with things like matrix concatenation and functions.</p>

<p>In addition to questions adapted from the MCS1, Will also asked the students about their feelings toward programming in general, and their confidence with the programming they had learned.
He found that MATLAB appeared to be little more than a glorified graphing calculator to most of the students.
The students’ collective mindset was exemplified nicely by this quote from one participant:</p>

<blockquote>
  <p>So I literally threw my calculator away and started doing everything on MATLAB.</p>
</blockquote>

<p>Most students did not foresee themselves using MATLAB in their futures, and felt that they had not gained generalisable ability with programming that they could use outside the classroom.
Some more quotes from students are below:</p>

<blockquote>
  <p>I know how to do explicitly what I’ve learned, and not much else.</p>
</blockquote>

<blockquote>
  <p>I think with Chemistry they do a really good job of teaching us. But there’s no, like, okay we’re only going to be doing like MATLAB and like learning the very basics of the basics.</p>
</blockquote>

<blockquote>
  <p>I should probably understand everything computationally little bit more because that’s the way that the research is moving and I think that’s, you know, an important part of the research future. So I want to develop these skills more but I’m like I don’t know what specific applications really look like.</p>
</blockquote>

<h2 id="final-remarks">Final remarks</h2>

<p>Our paper closes with some recommendations for teaching programming to these would-be end-user-programmers:</p>

<ul>
  <li><strong>Promote abstraction and abstract understandings</strong>. We discuss strategies for helping students achieve abstraction, for example, using multiple representations of the same concepts to help disentangle students’ understandings from the specific examples they have learned with.</li>
  <li><strong>Decomposition</strong>. Use sub-goals to break programming problems into small steps, and show examples of function reuse.</li>
  <li><strong>Meta-cognitive awareness</strong>. Teach an explicit programming problem-solving process so that students may confidently approach larger multi-step problems. This should include steps for problem comprehension, testing, and debugging.</li>
</ul>

<p>If you’re thinking that these recommendations would not be out of place in a book titled <em>How to Teach Programming to Anyone, Not Just These Chemistry &amp; Biochemistry Students</em>, you’re right!
These are all things that we try to accomplish in the introductory programming courses taken by our CS and SE majors.
But these skills as they relate to programming <a href="https://faculty.washington.edu/ajko/papers/Ko2011EndUserSoftwareEngineering.pdf">tend not to be prioritised</a> in end-user programming education.
However, though these students are not aiming to become software engineers, they <em>will</em> engineer software, potentially of a critical nature, and their programming education should account for this.</p>

<p>The good news is that upper-division Chemistry and Biochemistry students already have significant skill with abstraction, decomposition, and meta-cognitive awareness.
These skills are obviously not unique to computer science or programming—they are practised all the time in other STEM disciplines.
And so our final recommendation is to <strong>harness the students’ and instructors’ existing competencies</strong> with abstraction, decomposition, and meta-cognitive awareness while teaching them in the context of programming.</p>

<p>For example,</p>

<ul>
  <li><strong>Abstraction</strong> is already used when reasoning about chemical equations, which are lightweight abstractions of real-world phenomena.</li>
  <li><strong>Decomposition</strong> has been espoused as an important skill even in very early Chemistry education, e.g., scientific modelling skills involving breaking down a complex system into smaller elements and mechanisms.</li>
  <li><strong>Meta-cognitive awareness</strong> is required to successfully navigate the multi-step process involved in identifying a chemical compound’s structure from its spectroscopic data.</li>
</ul>

<p>The programming aspects of the Physical Chemistry courses are being revamped as I write this (including a transition to Python)—more updates soon!</p>

<hr />

<h2 id="postscript">Postscript</h2>

<p><strong>Why did we publish this paper at the Journal of Chemical Education, and not at a computing education venue like Koli Calling or SIGCSE?</strong></p>

<p>I am confident that this paper is published at the right venue, but it took a bit to get here.</p>

<p>The manuscript was rejected from computing education venues twice!
Both times, the primary (but not the only) hit against it was that it was probably of more interest to Chemistry instructors than CS instructors (the implication being that the readership of computing education research publications are mostly computing instructors).
I have two reasons to push back against this.
First, my view is that our Chemistry faculty who are teaching programming <em>are</em> computing instructors — they are teaching computing!
Second, I perhaps naïvely expect that if instructors in other disciplines are interested in research about teaching computing, they would look in computing education research publications for recommendations or techniques.</p>

<p>(Ok, fine, I also have some sense of professional identity wrapped up in publishing at my “home” venues.)</p>

<p>Meanwhile, as part of her broader work in computational Chemistry education, our co-author Ashley had been working with her colleagues to update the curriculum based on this work.
She’d been travelling to meetups of Chemistry educators and talking about this work, and reported high levels of interest and engagement from other Chemists working to include computing in their courses.
The audience for the paper was right there.</p>

<p>So the question was: do we choose a home for the paper based on its <strong>topic area</strong>, or its <strong>target audience</strong>?
In most cases, these point to the same venues.
But that’s not always true for works that straddle multiple academic disciplines.
Choosing a venue for a target audience may require less “convincing” to get it published, but choosing for a topic area may mean people can find it easily in the future.
I don’t know that there’s an obvious answer, but in this case there was certainly a practical one.</p>

<p>We made <em>significant improvements</em> to the manuscript (for example, we added the entire discussion section on abstraction, decomposition, and meta-cognitive awareness) and submitted it to the Journal of Chemical Education, where it ended up with extremely positive reviews.</p>

<p>I’m also grateful to be in a department where my scholarly output is not expected to exist in a narrow selection of publication targets—without this, there would’ve been a whole other calculus to consider.</p>

<hr />
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>At the time this work was carried out, the courses taught MATLAB, and are now switching to Python. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="research-summary" /><category term="intro-programming" /><summary type="html"><![CDATA[This is an overview of the research paper Recommendations for Improving End-User Programming Education: A Case Study with Undergraduate Chemistry Students, published in the American Chemical Society (ACS) Journal of Chemical Education.]]></summary></entry><entry><title type="html">It’s good to be bad at something</title><link href="https://ayaankazerouni.org/posts/good-to-be-bad" rel="alternate" type="text/html" title="It’s good to be bad at something" /><published>2024-03-31T00:00:00-07:00</published><updated>2024-03-31T00:00:00-07:00</updated><id>https://ayaankazerouni.org/posts/good-to-be-bad</id><content type="html" xml:base="https://ayaankazerouni.org/posts/good-to-be-bad"><![CDATA[<p><em>I signed up for a short machine shop training, and was reminded what it’s like to do something for the first time and struggle with it. In related news, I am teaching intro programming again in the Fall.</em></p>

<p>A couple of weeks ago some colleagues and I signed up for the Cal Poly College of Engineering <a href="https://ceng.calpoly.edu/connection/2019/09/red-tag-training-2/">Red Tag Tour</a> and earned our “red tags”.
These are passes that allow us to use the wood and sheet metal shop on the Cal Poly campus.
It was a 3-hour session that started with a whirlwind introduction to the machine shop in the Aero Hangar on campus, following which we worked through a scaffolded sequence of steps to make our own bookends.
Here’s mine:</p>

<figure class="wrap">
  <a href="/assets/posts/images/good-to-be-bad/bookend.png" target="_blank">
    
    <img src="/assets/posts/images/good-to-be-bad/bookend.png" alt="A bookend made by me in a machine shop training session." width="400px" />
    
  </a>

  <figcaption>Not my best handiwork, but my only handiwork.</figcaption>
</figure>

<p>It was a blast! I think it’s super cool that this is offered on our campus, and that they had a day where faculty could attend.
And I’m super grateful to the two student volunteers who were extremely patient with us.
What I do with my cool new red tag pass remains to be seen.</p>

<p>Anyway, we used a number of tools to make our bookends:</p>

<ul>
  <li>A compound slide to cut a piece of acrylic</li>
  <li>A few different drill presses</li>
  <li>A miter saw to cut a length of wood</li>
  <li>A step shear to cut a sheet of metal (by far the most fun tool to use)</li>
  <li>A rotex punch to punch holes in the metal</li>
  <li>A corner shear to cut a 90° corner off the metal</li>
  <li>A finger brake to bend the sheet metal</li>
</ul>

<p>It was a nicely-designed activity meant to introduce us to the types of tools that are available in a typical machine shop and what they do.</p>

<p>I had never set foot in a machine shop before, so this was all new to me. And boy was I bad at it.</p>

<p>By that I mean there were many steps with which I struggled in many little ways; ways that I didn’t expect while watching the student volunteer demonstrate the steps beforehand.
For example:</p>

<ul>
  <li>While cutting the acrylic piece, I stopped pushing the acrylic through the saw when I felt the resistance reduce. But I had only cut through 3/4 of the acrylic, and didn’t immediately see this from my line of sight. This was easy to fix, but resulted in a cut that was not super smooth.</li>
  <li>While countersinking the holes I drilled in my acrylic piece, I first drilled too little, and then over-corrected and ended up with comically large rims (far bigger than the heads of the screws that eventually went in).</li>
  <li>While punching holes in the sheet metal, I didn’t do so in a smooth motion and ended up with holes that were jagged on the other side. Thanks to the student volunteer who hammered away at them to flatten them out!</li>
</ul>

<p>It’s been a while since I did something <em>totally, completely</em> new to me.
It felt good to be that far from familiar territory!
First, I promptly forgot a fair bit about all the tools to which I was introduced.
Then, after watching the student volunteer demonstrate the steps to build the bookend, I thought “okay, looks fine, I can do that”.
Finally, I proceeded to struggle in a bunch of little ways that neither they nor I expected.</p>

<p><strong>The experience has made me a bit more empathetic toward students in my introductory programming courses.</strong>
For example, the students in my CS 0 class have often never programmed before I see them, and they are doing something <em>totally, completely</em> new to them.</p>

<p>A really reductive description of my intro course might be that I introduce students to a bunch of “tools” (<del>drill presses, step shear, miter saw</del> control flow, functions, test cases) and give them scaffolded practice tasks (<del>making a bookend</del> making data visualisations).
They then proceed to work on their bookends, and while doing so struggle in little ways that neither they nor I expected.</p>

<p>It’s hard to do things for the first time!</p>

<p>It’s also worth remembering that I don’t have any personal investment in being good at wood- or metal-working, so I was comfortable asking for help when I needed it, and was okay with taking a bit more time to make my bookend if I needed to.</p>

<p><strong>This is usually not so for first-year CS students.</strong>
At Cal Poly we have a “competitive enrolment policy”, i.e., students are admitted directly into the CS major, or need to jump through some GPA hoops to transfer into the CS major.
This can be a bad thing,<sup id="fnref:competitive-enrolment-policy" role="doc-noteref"><a href="#fn:competitive-enrolment-policy" class="footnote" rel="footnote">1</a></sup> in part because it results in students already having a bunch of perceived self-worth wrapped up in being good at computer science, which can get in the way of their learning.</p>

<p>Anyway, I’m grateful to the Red Tag training for (1) existing, and (2) reminding me what it’s like to be a student doing something for the first time.
I will try to remember the feeling during Q&amp;A sessions and office hours.</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:competitive-enrolment-policy" role="doc-endnote">
      <p>Nguyen &amp; Lewis. <em><a href="https://par.nsf.gov/servlets/purl/10195776">Competitive Enrollment Policies in Computing Departments Negatively Predict First-Year Students’ Sense of Belonging, Self-Efficacy, and Perception of Department</a></em> <a href="#fnref:competitive-enrolment-policy" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="intro-programming" /><summary type="html"><![CDATA[I signed up for a short machine shop training, and was reminded what it’s like to do something for the first time and struggle with it. In related news, I am teaching intro programming again in the Fall.]]></summary></entry><entry><title type="html">How relative novices make sense of code coverage and mutation-based feedback</title><link href="https://ayaankazerouni.org/posts/test-feedback-sensemaking" rel="alternate" type="text/html" title="How relative novices make sense of code coverage and mutation-based feedback" /><published>2024-01-04T00:00:00-08:00</published><updated>2024-01-04T00:00:00-08:00</updated><id>https://ayaankazerouni.org/posts/test-feedback-sensemaking</id><content type="html" xml:base="https://ayaankazerouni.org/posts/test-feedback-sensemaking"><![CDATA[<p>This is an overview of the research paper <em><a href="/publications#toce2023testing">A Model of How Students Engineer Test Cases with Feedback</a></em> published in ACM Transactions on Computing Education. The work was conducted at Cal Poly and was led by MS student <a href="https://www.linkedin.com/in/amshin98/">Austin Shin</a>.</p>

<h2 id="background">Background</h2>

<p>Most programming courses require students to write automated software tests to verify and demonstrate the correctness of their solutions to programming problems.
The quality of these test suites can vary in terms of their defect-detection capability.
The thoroughness of a test suite can be measured using a <em>test adequacy criterion</em> like <a href="https://en.wikipedia.org/wiki/Code_coverage">code coverage</a> or <a href="https://en.wikipedia.org/wiki/Mutation_testing">mutation analysis</a>.</p>

<p>In most CS courses, it is matter of course to assess both the correctness of students’ programmed solutions as well as the thoroughness of their software tests.
As students are encouraged to frequently write and run their own tests to check the correctness of their programs, so they are encouraged to frequently assess the thoroughness of their tests using one of the above criteria.</p>

<p>Within this context, it is useful to understand how students respond to software testing feedback while creating test suites.</p>

<h2 id="summary">Summary</h2>

<p>We qualitatively studied how students made sense of software testing feedback generated using two feedback mechanisms: code coverage and mutation analysis.</p>

<p>Our findings are summarised in the process model below.</p>

<figure>
  <a href="/assets/posts/images/test-feedback-sensemaking/process-model.png" target="_blank">
    
    <img src="/assets/posts/images/test-feedback-sensemaking/process-model.png" alt="A process diagram showing a novice's test selection process. A participant receives a testing task, then reads the source code or program description to first understand the problem and program. Once this is done, they write an initial set of tests based on experience and intuition. If any tests fail, the participant re-evaluates their understanding. Once all tests pass, they receive test adequacy feedback. They use strategies like code tracing to identify gaps in branch or mutation feedback. If mutation feedback is too difficult to address, the participant may fall back to addressing branch coverage instead. Once all feedback is satisfied, the testing task is done." />
    
  </a>

  <figcaption>A process model for how novices write software tests when being guided by a test adequacy criterion (code coverage or mutation analysis).</figcaption>
</figure>

<h2 id="method">Method</h2>

<p>We did a series of one-on-one interviews in which we gave students a number of small programs for which they were asked to write test cases.
Students were asked to think out loud while performing the testing tasks, and the sessions were recorded.</p>

<p>Interviews went roughly as follows:</p>

<ol>
  <li>Warm-up problem, with no testing feedback</li>
  <li>First testing problem, with no testing feedback</li>
  <li>Warm-up problem, with code coverage feedback</li>
  <li>Second testing problem, with code coverage feedback</li>
  <li>Warm-up problem, with mutation-based feedback</li>
  <li>Third testing problem, with mutation-based feedback</li>
</ol>

<p>Below is the interface in which students were given testing exercises.</p>

<figure class="wide">
  <a href="/assets/posts/images/test-feedback-sensemaking/muttle.png" target="_blank">
    <img src="/assets/posts/images/test-feedback-sensemaking/muttle.png" alt="A screenshot of the testing interface, showing where the user creates test cases and receives branch coverage or mutation coverage feedback about those test cases" />
  </a>

  <figcaption>Each exercise lets the user create, modify or delete test cases (A). If their test cases pass, they are given code coverage feedback in the coloured gutter next to the line numbers (B) or mutation-based feedback in the form of bug badges above the lines of code where mutations were made (C) In this example, the statement <code>return x * y</code> was mutated to <code>return x ** y</code>. The interviewer used the toggles at the top of the screen to switch between no coverage, code coverage, or mutation-based feedback, based on the experimental condition (D).</figcaption>
</figure>

<p>We qualitatively analysed the transcripts from these interviews.
For details about our analytic method, see <a href="/publications#toce2023testing">the paper</a>.</p>

<h2 id="results">Results</h2>

<p>Here are the highlights:</p>

<p><strong>Problem and program comprehension had a strong influence on students’ abilities to write useful tests.</strong>
If they had a shaky understanding of the problem or the code under test, their ability to address gaps in code coverage or mutation coverage suffered.</p>

<p><strong>Various intuitions came into play when no feedback was available.</strong>
Nearly all students started with a “happy path” test case—something simple that they could quickly work out in their minds.
They may have been using these simple test cases as scaffolds to confirm that they understood the problem and program correctly.</p>

<p>When no testing feedback was available, students often chose test inputs based on intuitions about “edge cases”—these most often took the form of boundary values for the data type at hand (e.g., zero, negative numbers, or empty lists).
Importantly, these types of inputs were chosen whether or not they represented unexplored <a href="https://en.wikipedia.org/wiki/Equivalence_partitioning">equivalence partitions</a> in the input space.</p>

<p>Some students started by identifying beacons<sup id="fnref:beacons" role="doc-noteref"><a href="#fn:beacons" class="footnote" rel="footnote">1</a></sup> in the problem description or program, and targeting their initial tests toward those beacons.
For example, in the <em>Rainfall problem</em>, the program is given an input list of numbers (daily rainfall), and is expected to compute the mean of all the positive numbers that occur before a “sentinel value” (say, 99999).
In our interviews, most students zeroed in on that sentinel value requirement and wrote an early test to target that requirement.</p>

<p>Finally, because students had seen code coverage before, some of them mentally simulated code coverage to self-assess their own test suites and identify gaps.</p>

<p><strong>Code tracing strategies were employed while addressing code coverage feedback.</strong>
As described above, code comprehension played an important role in students’ abilities to address coverage gaps.
They used various strategies to manage the cognitive load of code comprehension involved during testing.
For example,</p>

<ul>
  <li>While tracing code to identify gaps in (code or mutation) coverage, students often limited their tracing to the <em>basic block</em><sup id="fnref:basic-block" role="doc-noteref"><a href="#fn:basic-block" class="footnote" rel="footnote">2</a></sup> in which the gap existed.</li>
  <li>They used <em>variable roles</em><sup id="fnref:variable-roles" role="doc-noteref"><a href="#fn:variable-roles" class="footnote" rel="footnote">3</a></sup> to help them reason about the variables involved their tracing.</li>
  <li>Sometimes, they simply ignored the feedback and opted to write tests based on their intuitions.</li>
</ul>

<p><strong>Addressing mutation-based feedback proved to be cognitively demanding.</strong>
Reasoning about mutation-based feedback appeared to be a high-cognitive-load activity for the interviewed students.
To devise test cases to address a gap in mutation coverage, students needed to develop and maintain an understanding of the mutated program <em>while simultaneously</em> maintaining their understanding of the original program.
Moreover, they need to identify to point at which the two programs diverge.
This was a demanding task.</p>

<p>Even after demonstrating an understanding of the idea behind mutation analysis, students struggled mightily on certain mutants.
One student was so distracted by this parallel comprehension task that when they eventually wrote a test case, they wrote one that would <em>pass</em> the mutated program, and <em>fail</em> the original program (i.e., the opposite of the task at hand).</p>

<p>Difficulties more commonly arose for mutants that appeared at conditional branching points in the program, as opposed to, say, mutants that involved changes to arithmetic expressions or variable assignments.</p>

<p>As before, students developing strategies to manage this demanding parallel code tracing task:</p>
<ul>
  <li>Like with code coverage, they traced basic blocks and reasoned about the program in terms of variable roles.</li>
  <li>When addressing mutation-based feedback was too difficult, they “fell back” to addressing the weaker criterion (code coverage) instead. This heuristic—of simply targeting code coverage instead of mutants—was sometimes fruitful.</li>
  <li>Finally, some students ignored the specific mutations and focused only on the fact that they were <em>present</em>. The presence of an un-addressed mutant alerted students to “suspicious” lines of code, and they did not need to look at the specific mutation in order to target their testing toward those lines.</li>
</ul>

<p>For more details, see <a href="/publications#toce2023testing">the paper</a>.</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:beacons" role="doc-endnote">
      <p>Prominent structures or symbols (variable names, function names, comments) in a program (or problem description, here) that help a reader to quickly understand the program’s purpose. <a href="#fnref:beacons" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:basic-block" role="doc-endnote">
      <p>A basic block in a program is a straight-line sequence of statements such that “if the first statement is executed, all statements in the block will be executed”. <a href="#fnref:basic-block" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:variable-roles" role="doc-endnote">
      <p><a href="https://ieeexplore.ieee.org/document/1046340">Sajaniemi</a> suggests that a small number of categories can describe the purposes of most variables in most programs. When novice programmers were explicitly taught to recognise these categories, their performance on code comprehension tasks improved significantly. This general idea has a <a href="https://www.manning.com/books/the-programmers-brain">strong basis in human cognition</a>, even if the specific categories suggested are are bit limited to imperative programming languages. <a href="#fnref:variable-roles" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="research-summary" /><category term="software-testing" /><summary type="html"><![CDATA[This is an overview of the research paper A Model of How Students Engineer Test Cases with Feedback published in ACM Transactions on Computing Education. The work was conducted at Cal Poly and was led by MS student Austin Shin.]]></summary></entry><entry><title type="html">What makes CS students seek or avoid academic help resources?</title><link href="https://ayaankazerouni.org/posts/help-seeking-behaviours" rel="alternate" type="text/html" title="What makes CS students seek or avoid academic help resources?" /><published>2021-11-13T00:00:00-08:00</published><updated>2021-11-13T00:00:00-08:00</updated><id>https://ayaankazerouni.org/posts/help-seeking-behaviours</id><content type="html" xml:base="https://ayaankazerouni.org/posts/help-seeking-behaviours"><![CDATA[<p><small>
<em>This article was originally posted on <a href="https://ayaankazerouni.medium.com">my Medium blog</a> on November 13, 2021.</em>
</small></p>

<p>This is an overview of the paper <em><a href="https://ayaankazerouni.github.io/publications#koli2021help-seeking">Patterns of Academic Help-Seeking in Undergraduate Computing Students</a></em>, appearing at the 2021 <a href="https://www.kolicalling.fi/">Koli Calling</a> conference on computing education research. It was written by my student collaborator Augie Doebling and myself.</p>

<p>Help-seeking is an expected phase in learning or problem-solving. The process involves a fair bit of self-regulatory skill; a learner must recognise that a problem or difficulty exists, assess whether they need help to surmount it, identify a help resource, and finally seek and process help.</p>

<p>Undergraduate students tend to have a variety of academic help resources at their disposal. For example, taking Cal Poly as a typical example, students can seek help with their coursework from online sources, their peers, instructors, or the departmental peer tutoring centre.</p>

<p>Much has been written about how students use individual resources, such as <a href="https://dl.acm.org/doi/10.1145/3291279.3339418">TA office hours</a> or <a href="https://dl.acm.org/doi/10.1145/3017680.3017745">Piazza</a>. But what we know holistically about how computing students navigate this array of resources is largely anecdotal. What resources do they tend to use most frequently? Does this differ for different demographic groups? What influences students to approach or avoid certain resources?</p>

<p>We conducted a mixed-methods study to better understand the help-seeking behaviours of students in the CSSE department at Cal Poly.</p>

<ul>
  <li><strong>Survey:</strong> Distributed in a wide variety of Cal Poly CS courses, asking students about the frequency with which they accessed various help resources.</li>
  <li><strong>Interviews:</strong> A series of one-on-one interviews with students to learn the factors that influence their help-seeking decisions.</li>
</ul>

<p>We discussed the following resources (acronyms are for the figure that follows):</p>

<ul>
  <li>The instructor—in office hours (IN-OH), in class (IN-CL), or online (IN-OC)</li>
  <li>The TA—in class (TA-CL) or online (TA-OC)</li>
  <li>Peers—enrolled in the same class (PEC) or other classes (OP)</li>
  <li>The peer tutoring centre (CSTC)</li>
  <li>Online materials—specific to the course (OM-SC) or not specific to the course (OM-NSC)</li>
</ul>

<h2 id="frequency-of-help-seeking">Frequency of help-seeking</h2>

<p>We received 138 survey responses about the frequency with which students accessed various help resources.</p>

<div style="background-color: grey;">

<figure>
  <a href="/assets/posts/images/help-seeking-behaviours/frequency.png" target="_blank">
    
    <img src="/assets/posts/images/help-seeking-behaviours/frequency.png" alt="" />
    
  </a>

  <figcaption>Frequency of accessing various help resources.</figcaption>
</figure>

</div>

<p>Students most frequently relied on <strong>online sources</strong>, followed closely by their <strong>peers in class</strong>.
They reported modest reliance on the <strong>instructor</strong> for help, preferring to ask questions in class or online rather than going to office hours.</p>

<p>Students did not report much use of <strong>course TAs</strong> or the <strong>peer tutoring centre</strong>.
The former may be because TAs at Cal Poly do not hold office hours like they might at other universities—there is typically much more contact with instructors than with TAs.</p>

<h2 id="trends-by-student-demographics">Trends by student demographics</h2>

<p>There was no difference in the overall frequency of help-seeking (across all resources) between men and women.
However, women reported turning to the “social” help resources more often than men did: they attended instructor office hours and accepted help from peers roughly <em>Once a Week</em> compared to men’s <em>Every Few Weeks</em>.</p>

<p><strong>Why might this be?</strong> <a href="https://bera-journals.onlinelibrary.wiley.com/doi/10.1002/rev3.3196">Previous research</a> has suggested that women tend to have a better attitude toward help-seeking than men do, viewing it more as a learning strategy and less as a sign of dependence.
It’s possible that students who are less inclined to seek help from social sources perceive some level of threat to their self-esteem from the act of seeking help.</p>

<p>Finally, computing majors reported relying on their peers and online sources more often than did non-computing majors. No other notable patterns were observed for other resources and demographic groups (e.g., ethnicity, prior experience with computing, or academic progress).</p>

<h2 id="what-makes-students-approach-or-avoid-help-resources">What makes students approach or avoid help resources?</h2>

<p>Having discovered trends and correlations regarding frequency of accessing help resources, we turned to interviews to understand why students make the help-seeking decisions that they do.</p>

<p>We asked students about the primary resources they turn to for academic help, the resources that they tend to avoid, and their reasons for both. A number of themes emerged from our qualitative analysis of interview transcripts.</p>

<p>First, students tended to progress from informal to formal sources of help: a frequently reported pattern was the progression from <em>online sources</em> to <em>peers</em> to <em>instructors</em>, where students only progressed to the next resource if the previous resource did not help surmount a problem.</p>

<p>Below, I describe students’ reasons for using or not using different sources of academic help.</p>

<h3 id="online-sources">Online sources</h3>

<p>Reasons for using:</p>

<ul>
  <li><em>Ease of access:</em> Just a few button presses away.</li>
  <li><em>Concrete examples:</em> Sites like StackOverflow were reported as being useful when one is trying to “get the ball rolling” with a new language or API, but less useful for obtaining concept knowledge about a topic.</li>
</ul>

<p>Reasons for not using:</p>

<ul>
  <li><em>Low signal-to-noise ratio:</em> It takes experience and expertise to sort through the wealth of information available through a simple Google search. Students reported that online resources became more useful to them as they became more experienced programmers, but were overwhelming when they were first learning programming.</li>
</ul>

<p>Interestingly, some students did not report online sources in their help-seeking process until specifically asked about them; they did not view it as seeking help, but rather viewed it as helping themselves.</p>

<h3 id="peers">Peers</h3>

<p>Reasons for using:</p>

<ul>
  <li><em>Ease of access:</em> Peers are just a text message away.</li>
  <li><em>Stress-free help:</em> Peers tend to not judge one’s lack of knowledge. There is less (perceived or actual) “threat” from seeking help from a peer than there is from, say, course staff.</li>
</ul>

<p>Reasons for not using:</p>

<ul>
  <li><em>Lack of a peer network:</em> First-year students, transfer students, or students from historically minoritised groups may not have access to a solid network of peers.</li>
  <li><em>Fear of academic dishonesty:</em> Students worried about accidentally breaking rules related to academic dishonesty if they worked too closely with peers. For example, they reported being hesitant to speak in detail about (or ask others about) their programming projects.</li>
</ul>

<h3 id="instructors">Instructors</h3>

<p>Reasons for using:</p>

<ul>
  <li><em>Depth of content knowledge:</em> Instructors are knowledgeable about their subject matter, and often provide the definitive help needed to surmount a problem.</li>
  <li><em>Depth of pedagogical content knowledge:</em> Instructors have seen many of the common bugs, pitfalls, and strategies used for their assignments, and are best suited to help when a student is struggling.</li>
  <li><em>Forming a connection:</em> Synchronous office hours helped some students form connections with their instructors, making help-seeking and learning a stress-free experience.</li>
</ul>

<p>Reasons for not using:</p>

<ul>
  <li><em>Tacit knowledge:</em> Instructors often have an “expert blind spot”, and the help they give often assumes knowledge that the student (1) doesn’t have, or (2) can’t automatically transfer to their current problem.</li>
  <li><em>Approachability:</em> Students reported that instructors could often be intimidating (or worse, demeaning) when they were asked for help. Importantly, many students reported that poor experiences with one instructor made them less likely to seek help from any instructors in the future.</li>
</ul>

<h2 id="recommendations-for-cs-instructors-or-departments">Recommendations for CS instructors or departments</h2>

<p>Based on students’ responses, we close with some recommendations for reducing the barriers to seeking academic help.</p>

<ul>
  <li><strong>Online sources</strong>—In early courses, model a process for finding information online and identifying high-quality sources. This need not be limited to StackOverflow answers; it can also include official documentation for programming languages or APIs.</li>
  <li><strong>Peers</strong>—Feature collaborative work more prominently in early courses. This could include team projects as well as teaching practices like peer instruction or think-pair-share. Featuring more collaborative work in earlier courses would provide students access to peers to work with “legally”, and would help them form peer networks that could last into future courses.</li>
  <li><strong>Instructors</strong>—Ensure that classrooms and office hours are welcoming spaces. Instructors play a massive role in shaping the overall climate of a classroom or department. Being cognizant of this outsized impact and taking steps to ensure a welcoming atmosphere could have huge positive implications for student success.</li>
</ul>]]></content><author><name></name></author><category term="research-summary" /><category term="help-seeking" /><summary type="html"><![CDATA[This article was originally posted on my Medium blog on November 13, 2021.]]></summary></entry><entry><title type="html">Fast and Accurate Incremental Feedback for Students’ Software Tests Using Selective Mutation Analysis</title><link href="https://ayaankazerouni.org/posts/fast-accurate-mutation-feedback" rel="alternate" type="text/html" title="Fast and Accurate Incremental Feedback for Students’ Software Tests Using Selective Mutation Analysis" /><published>2021-03-17T00:00:00-07:00</published><updated>2021-03-17T00:00:00-07:00</updated><id>https://ayaankazerouni.org/posts/fast-accurate-mutation-feedback</id><content type="html" xml:base="https://ayaankazerouni.org/posts/fast-accurate-mutation-feedback"><![CDATA[<p><small>
<em>This post originally appeared on <a href="https://ayaankazerouni.medium.com">my Medium blog</a> on March 17, 2021.</em>
</small></p>

<p>This is an overview of the paper <em><a href="https://www.sciencedirect.com/science/article/pii/S0164121221000029">Fast and Accurate Incremental Feedback for Students’ Software Tests Using Selective Mutation Analysis</a></em>, published in the Journal of Systems and Software. The paper is freely available. My co-authors were <a href="https://davisjam.github.io">Jamie Davis</a>, <a href="https://arinjoy-basak.github.io/">Arinjoy Basak</a>, <a href="https://people.cs.vt.edu/shaffer">Cliff Shaffer</a>, <a href="https://people.cs.vt.edu/fservant">Francisco Servant</a>, and <a href="https://people.cs.vt.edu/edwards">Steve Edwards</a>.</p>

<p>TL;DR: Use the <code class="language-plaintext highlighter-rouge">RemoveConditionals</code> and arithmetic operator deletion (<code class="language-plaintext highlighter-rouge">AOD</code>) mutation operators for fast and reliable mutation analysis.</p>

<h2 id="summary">Summary</h2>

<p>Feedback about students’ software tests is often generated using code coverage criteria (like statement or condition coverage). These can be unreliable given that code coverage is satisfied simply by the execution of the code-under-test, and not by the actual assertions in the tests.</p>

<p><em>Mutation analysis</em> is a stronger but much more costly criterion for measuring the adequacy of software tests. In this paper, we evaluated the feasibility of existing approaches to mutation analysis for producing automated feedback for student-written software tests. After finding that existing approaches were infeasible, we proposed new approaches for fast and accurate mutation analysis. Finally, we evaluated our proposed approaches for validity on an external dataset of open-source codebases, and report that our results may be generalisable beyond our educational context.</p>

<p>This post is of interest to Computer Science educators interested in giving students useful feedback about their software testing, and to software engineers interested in using mutation analysis to help them write stronger software tests.</p>

<h2 id="background">Background</h2>

<p>Software testing is important. As it is increasingly incorporated into undergraduate programming courses, teachers are giving students feedback not only about the correctness of their programs, but also about the quality of their software tests.</p>

<p>Much of this feedback is based on assessments of test adequacy, most commonly <em>code coverage criteria</em>.</p>

<p>Code coverage criteria are satisfied when structural elements (statements, conditions, etc.) of a program are exercised by a test suite at least once. For example, under statement coverage, the test suite’s adequacy is measured as the percentage of program constructs that are executed by the tests. Code coverage is <strong>fast to compute</strong> and <strong>amenable to incremental feedback</strong>. But it can be <strong>unreliable</strong>, because the criterion is not bound to the <em>assertions</em> in software tests, just to the underlying code that is executed.</p>

<p><em><a href="https://en.wikipedia.org/wiki/Mutation_testing">Mutation analysis</a></em> is a far more reliable option. Small changes (<em>mutations</em>) are made to the target program, creating incorrect variants called <em>mutants</em>. The test suite is run against these mutants, and its adequacy is measured as the percentage of mutants that are detected by the test suite (i.e., by a failing test). The different kinds of mutations you could make are called <em>mutation operators</em>.</p>

<p>Mutation analysis subsumes code coverage as a test adequacy criterion<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, and has been shown to be a <strong>reliable measurement of test adequacy</strong>. It can also be used to produce <strong>incremental feedback</strong>: I don’t need a student to have finished a project to give them feedback about their tests.</p>

<h2 id="reducing-the-cost-of-mutation-analysis">Reducing the cost of mutation analysis</h2>
<p>Unfortunately, mutation analysis can be <strong>prohibitively expensive</strong> computationally. The number of mutants produced for even a moderately sized project (~1 KLoC) can reach well into the thousands. Running the test suite for each of these mutants can take several minutes, sometimes hours.</p>

<p>Significant research effort has been devoted to reducing this cost. One such approach is <em>selective mutation</em>. The main idea behind selective mutation is to select a subset of mutation operators that give you the best “bang for your buck”. That is, out of the <strong>Full</strong> set of mutation operators (all available operators) you want a subset that gives you a reliable test adequacy score — one that is close to the “true” thoroughness of your tests — while producing a small number of mutants.</p>

<p>Numerous such operator subsets have been proposed. One key example is the <strong>Deletion</strong> set, originally proposed by Roland Untch<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> and reified by Jeff Offutt and colleagues<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</p>

<p>Deletion operators create mutants by systematically deleting program constructs. This simple mutation scheme results in significantly fewer mutants. For example, if we were to remove the arithmetic operator in the expression <code class="language-plaintext highlighter-rouge">a + b</code>, we just create two mutants: <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>. This is in contrast to the four mutants that would be created by arithmetic operator <strong>replacement</strong>: <code class="language-plaintext highlighter-rouge">a — b, a / b, a * b, a % b</code>.</p>

<p>We used the mutation analysis system <a href="https://pitest.org">PIT</a>, which is built for speed and scalability. We approximated the Deletion set in PIT to be:</p>

<ul>
  <li><strong>RemoveConditionals</strong>, which replaces conditional statements with boolean literals, i.e., <code class="language-plaintext highlighter-rouge">true</code> (forcing the execution of the “if” branch) or <code class="language-plaintext highlighter-rouge">false</code> (forcing execution of the “else” branch)</li>
  <li><strong>Arithmetic operator deletion (AOD)</strong>. E.g., the expression <code class="language-plaintext highlighter-rouge">a + b</code> would produce the mutants <code class="language-plaintext highlighter-rouge">a</code> and <code class="language-plaintext highlighter-rouge">b</code>, removing the arithmetic operator (and one operand) entirely.</li>
  <li><strong>NonVoidMethodCalls</strong>, which replaces calls to non-void methods with the default values for the specific return type. That is, <code class="language-plaintext highlighter-rouge">int</code>-returning method calls would be replaced with 0, <code class="language-plaintext highlighter-rouge">Object</code>-returning method calls would be replaced with <code class="language-plaintext highlighter-rouge">null</code>, etc.</li>
  <li><strong>VoidMethodCalls</strong>, which simply removes calls to methods that do not return anything.</li>
  <li><strong>MemberVariable</strong>, which removes assignments to instance variables.</li>
  <li><strong>ConstructorCalls</strong>, which replaces calls to constructors with <code class="language-plaintext highlighter-rouge">null</code>.</li>
</ul>

<h2 id="fast-and-accurate-mutation-based-feedback">Fast and accurate mutation-based feedback</h2>

<p>Students in our courses are allowed and encouraged to make many incremental submissions to the auto-grader to help them ensure they’re on the right track. As deadlines approach, this can result in bursty traffic placing enormous load on the server.</p>

<p>We ran mutation analysis on 1389 submissions in two courses in the CS program at Virginia Tech: a second year course on Software Design and Data Structures, and a third-year course on Data Structures and Algorithms. Projects were implemented in Java, and students were required to turn in <a href="https://junit.org">JUnit</a> test suites with their project submissions.</p>

<p>Analysis was conducted on a machine with similar specifications as the one serving our auto-grading infrastructure, <a href="https://web-cat.github.io">Web-CAT</a>. We did not eliminate <em>equivalent mutants</em> from our data-set. These cannot be automatically identified, which makes eliminating them from our corpus a daunting prospect.</p>

<p>We grouped submissions by source lines of code (SLoC), ranging from ~150 LoC to ~1200 LoC.</p>

<figure>
  <a href="/assets/posts/images/fast-accurate-mutation-feedback/submission-groups.png" target="_blank">
    
    <img src="/assets/posts/images/fast-accurate-mutation-feedback/submission-groups.png" alt="A histogram showing project groupings. Group 1, with submissions smaller than 341 lines of code, contains 672 submissions. Group 2, with submissions smaller than 666 lines of code, contains 353 submissions. Group 3, with submissions smaller than 1097 lines of code, contains 245 submissions. Group 4, with the largest submissions, contains 119 lines of code." />
    
  </a>

  <figcaption>Groups of submissions based on source lines of code (SLoC). Dashed lines indicate group boundaries.</figcaption>
</figure>

<p>For each operator subset, we looked at</p>
<ul>
  <li><strong>Computational cost:</strong> the running time in seconds (i.e., how long would a student spend twiddling their thumbs waiting for feedback?) and the number of mutants produced per thousand lines of code</li>
  <li><strong>Accuracy</strong> at predicting coverage under the Full set. That is, if mutants under a given subset are killed by the test suite, how likely is it that the test suite will also kill mutants under the Full set?</li>
</ul>

<p>Preliminary results showed that comprehensive mutation (i.e., using all available mutation operators) was certainly too slow for our purposes. For submissions in the larger submissions groups, even the Deletion set took too long (nearly a minute) to produce feedback.</p>

<p>That said, the Deletion set showed promise. As Offutt and friends reported, it produces a remarkably good approximation of mutation adequacy at a fraction of the computational cost of comprehensive mutation.</p>

<p>Can we reduce this cost further?</p>

<h2 id="reducing-the-cost-of-the-deletion-set">Reducing the cost of the Deletion set</h2>
<p><em>Do we need all six Deletion operators to make a useful approximation of mutation adequacy?</em></p>

<p>We used forward selection to determine an appropriate ordering of Deletion operators, set up as follows:</p>

<ul>
  <li><strong>Dependent variable:</strong> Mutation coverage using all available operators</li>
  <li><strong>Independent variables:</strong> Mutation coverage under each individual Deletion operator</li>
  <li><strong>Procedure:</strong> Starting from an empty set of operators, we iteratively added the single operator that most improved a model predicting comprehensive mutation coverage, stopping when all Deletion operators were included or when the model could no longer improve.</li>
</ul>

<p>All Deletion operators were included, in the following order: <code class="language-plaintext highlighter-rouge">RemoveConditionals, AOD, NonVoidMethodCalls, VoidMethodCalls, MemberVariable, ConstructorCalls</code></p>

<p>Then, we examined the cost and effectiveness of incremental slices of this ordering:</p>

<ul>
  <li><strong>1-operator subset</strong>, containing only <code class="language-plaintext highlighter-rouge">RemoveConditionals</code></li>
  <li><strong>2-operator subset</strong>, containing <code class="language-plaintext highlighter-rouge">RemoveConditionals</code> and <code class="language-plaintext highlighter-rouge">AOD</code></li>
  <li><strong>3-operator subset</strong>, containing <code class="language-plaintext highlighter-rouge">RemoveConditionals</code>, <code class="language-plaintext highlighter-rouge">AOD</code>, and <code class="language-plaintext highlighter-rouge">NonVoidMethodCalls</code></li>
  <li>…continued until the entire Deletion set is included</li>
</ul>

<h2 id="result">Result</h2>
<p>We found that most of the Deletion set’s effectiveness comes from the first two operators, i.e., <code class="language-plaintext highlighter-rouge">RemoveConditionals</code> and <code class="language-plaintext highlighter-rouge">AOD</code>. Inclusion of additional operators drives up the cost, but with little improvement to accuracy.</p>

<figure class="wide">
  <a href="/assets/posts/images/fast-accurate-mutation-feedback/inc-subsets.png" target="_blank">
    <img src="/assets/posts/images/fast-accurate-mutation-feedback/inc-subsets.png" alt="Four subplots, each with four box plots showing operator subset cost, and line plots showing operator subset accuracy." />
  </a>

  <figcaption>The cost and accuracy of our proposed incremental subsets of operators. For each subplot, the left axis represents cost (# mutants per KSLoC) and the right axis represents accuracy (Adjusted R² predicting Full coverage) Y-axes are shared across subplots. Inline text at the bottom of the charts indicates the median running time on our server.</figcaption>
</figure>

<p>In the figure above,</p>
<ul>
  <li>Each column represents a single submission group,</li>
  <li>Each box plot represents the cost distribution of a subset for that submission group, in mutations-per-thousand-LoC,</li>
  <li>Each blue dot represents the percent of variance in comprehensive mutation that is explained by that subset for the given submission group</li>
</ul>

<p>We can see that for submission groups 2–4, the cost drops precipitously from Deletion \(\to\) 3-op \(\to\) 2-op \(\to\) 1-op, while the accuracy stays more or less the same. For the smaller submissions in group 1, it’s possible that they simply do not provide enough opportunities for mutation to take place, so accuracy takes a huge hit for each mutation operator that is excluded.</p>

<p>Some key takeaways:</p>

<p><strong>The RemoveConditionals operator, by itself, is enormously effective</strong> for the larger, more complex submissions, pushing 90% adjusted \(R^2\) for group 4 submissions (see the 1-op box plot in the rightmost subplot). For groups 2 and 3, it still does pretty well, but requires the inclusion of <code class="language-plaintext highlighter-rouge">AOD</code> operator to cross the 90% threshold.</p>

<p><strong>Which operators are most useful seems tied to the project itself.</strong> It is no surprise that <code class="language-plaintext highlighter-rouge">RemoveConditionals</code> does not do so well for the group 1 submissions: they are of minimal cyclomatic complexity, meaning they contain few conditional statements. Including <code class="language-plaintext highlighter-rouge">AOD</code> substantially improves the approximation, because these projects tend to focus more on arithmetic operations and less on data structure implementations (in contrast to the submissions found in the Data Structures and Algorithms course).</p>

<h2 id="validating-our-results">Validating our results</h2>
<p>At this point, it looks like mutation analysis using <code class="language-plaintext highlighter-rouge">RemoveConditionals</code> or <code class="language-plaintext highlighter-rouge">RemoveConditionals+AOD</code> are feasible options for giving our students fast and reliable feedback about their test suites.</p>

<p>The question now is: are these results generally useful? Or are they specific to submissions produced by students in our courses?</p>

<p>We turn to a data-set<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> released by Marinos Kintis and colleagues containing programs, mutants, and mutation-adequate test suites, drawn from 6 open-source Java projects.</p>

<p>Using this dataset, we evaluated the cost and effectiveness of the Deletion, 3-op, 2-op, and 1-op subsets.</p>

<p>For each subset, we examined:</p>
<ul>
  <li><strong>Reliability</strong>, measured by creating a subset-adequate test suite, and seeing how it held up using all possible mutants. In other words, if a developer stopped testing when they satisfied a subset, how thorough would their test suite be under comprehensive mutation?</li>
  <li><strong>Cost</strong>, measured as the percentage of all possible mutants that were created by the subset.</li>
</ul>

<p>Kintis et al. also hand-marked equivalent mutants in their published data-set. This gives us an opportunity to test our operator subsets in the absence of these mutants, addressing a limitation present in our analysis of our students’ submissions.</p>

<h2 id="result-1">Result</h2>
<p>Results were largely in agreement with the study described above.</p>

<p>In terms of <strong>reliability</strong>, it appeared that the incremental subsets were nearly as effective as the entire Deletion set, with the 3-op subset (<code class="language-plaintext highlighter-rouge">NonVoidMethodCalls</code> and beyond) bringing diminishing returns. This is similar to our original results.</p>

<figure>
  <a href="/assets/posts/images/fast-accurate-mutation-feedback/val-mutation-score.png" target="_blank">
    
    <img src="/assets/posts/images/fast-accurate-mutation-feedback/val-mutation-score.png" alt="Four box-plots shows the Mutation score of the Deletion, 3-op, 2-op, and 1-op subsets. The median mutation scores are 0.95, 0.95, 0.95, and 0.9, respectively." />
    
  </a>

  <figcaption>Mutation coverage: Proportion of Full mutants detected by the subset-adequate test suite.</figcaption>
</figure>

<p>The <strong>cost</strong> naturally decreases in the order Deletion \(\to\) 3-op \(\to\) 2-op \(\to\) 1-op.</p>

<figure>
  <a href="/assets/posts/images/fast-accurate-mutation-feedback/val-cost.png" target="_blank">
    
    <img src="/assets/posts/images/fast-accurate-mutation-feedback/val-cost.png" alt="The cost in terms of the proportion of mutants produced by the Deletion, 3-op, 2-op, and 1-op subsets. The median proportions are around 0.155, 0.148, 0.110, 0.060, respectively." />
    
  </a>

  <figcaption>Computational cost: Number of mutants produced by each subset, expressed as a proportion of the Full number of mutants.</figcaption>
</figure>

<h2 id="final-remarks">Final remarks</h2>
<p>These results suggest that the <code class="language-plaintext highlighter-rouge">RemoveConditionals</code> operator is a feasible option for fast and accurate mutation analysis. And this makes sense, because <code class="language-plaintext highlighter-rouge">RemoveConditionals</code> can be thought of as a stronger form of condition coverage—only instead of simply requiring that all conditions evaluate to true and false at least once, it is satisfied when the tests depend on the conditions evaluating to true or false at least once. The difference is subtle, but results in much more thorough tests when used as a basis for measuring test adequacy.</p>

<p>Including the <code class="language-plaintext highlighter-rouge">AOD</code> operator provides an even stronger criterion, and it especially useful when the code-under-test has few logical branches. Including further Deletion operators drives up the cost but without improvements in effectiveness.</p>

<p>It remains to be seen whether mutation-based testing feedback using one or both of these operators helps students to produce stronger test suites. Future work should involve evaluating mutation analysis for its utility as a device for practice and feedback with software testing.</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://cs.gmu.edu/media/techreports/ISSE-TR-96-01.pdf">Offutt, 1996</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://dl.acm.org/doi/10.1145/1566445.1566540">Untch, 2009</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><a href="https://ieeexplore.ieee.org/document/6823861">Delamaro, Offutt, &amp; Ammann, 2014</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><a href="http://pages.cs.aueb.gr/~kintism/papers/scam2016/">Kintis et al., 2016</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="research-summary" /><category term="software-testing" /><summary type="html"><![CDATA[This post originally appeared on my Medium blog on March 17, 2021.]]></summary></entry><entry><title type="html">Explicit Milestones on Intermediate Software Projects are Pretty Not Bad</title><link href="https://ayaankazerouni.org/posts/explicit-project-milestones" rel="alternate" type="text/html" title="Explicit Milestones on Intermediate Software Projects are Pretty Not Bad" /><published>2020-10-14T00:00:00-07:00</published><updated>2020-10-14T00:00:00-07:00</updated><id>https://ayaankazerouni.org/posts/explicit-project-milestones</id><content type="html" xml:base="https://ayaankazerouni.org/posts/explicit-project-milestones"><![CDATA[<p><small>
<i>This article was originally posted on <a href="https://ayaankazerouni.medium.com">my Medium blog</a> on October 14, 2020.</i>
</small></p>

<p><em>This is an overview of the research paper “<strong><a href="/publications#sigcse2021milestones">The Impact of Programming Project Milestones on Procrastination, Project Outcomes, and Course Outcomes</a></strong>” by <a href="https://people.cs.vt.edu/shaffer">Cliff Shaffer</a> and myself, appearing at SIGCSE 2021.</em></p>

<h2 id="summary">Summary</h2>

<p>Undergraduate CS students often lack experience working on large, un-scaffolded, and long-running software projects. When they encounter such projects for the first time in intermediate programming courses, they do so without any procedural knowledge for how to go about tackling them. As a result, many of them struggle to complete these projects on time or correctly. As educators, we can do a better job supporting students as they approach these projects.</p>

<p>We started giving students explicit project milestones with intermediate deadlines, to go with software project specifications in a third-year Data Structures course. The goal is to give students guided practice with project decomposition, time management, and successful project completion.</p>

<p>In a quasi-experiment to determine the impact that milestones had on timeliness, project correctness, and course outcomes, we found that:</p>
<ul>
  <li>Milestones had a considerably strong effect on timeliness, reducing the rate of late submissions by nearly 30%</li>
  <li>Students with milestones produced projects with slightly higher correctness than students without milestones</li>
  <li>Milestones had little effect on frequencies of course outcomes, with its positive effects limited to the “students in the middle”. Unfortunately, just as many students failed the course, and just as many students withdrew from the course, with or without milestones.</li>
</ul>

<p>Read on for more information.</p>

<h2 id="background">Background</h2>

<p>Students in intermediate programming courses often work on large, un-scaffolded, and relatively long-running software projects. These tend to be larger and more complex than projects they have worked on in the past, and consequentially bring with them a set of self-regulatory challenges that the students may be ill-equipped to deal with. No longer are assignment specifications given as easy-to-approach bullet-lists of requirements. Instead, they’re presented with a “wall of prose” from which they must extract requirements, plan their development, and get to work.</p>

<p>An expert software developer has the domain-specific self-regulatory skills to tackle such a project. She can decompose such a project into sub-goals and budget the time needed to tackle each one. Novices, however, are often unable to do this decomposition themselves, and are famously poor at estimating the time needed for a development task <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. So instead, they are left facing the project with no clear place to start, low self-efficacy regarding their ability to finish, and not much prior experience from which to draw strategies or confidence.</p>

<p>A common manifestation of students’ ill-preparedness to approach these projects is procrastination, the “quintessential self-regulatory failure”<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. According to Science, there are many reasons for procrastination. Chief among them are task-related reasons. For example, we know that people are more likely to procrastinate:</p>
<ul>
  <li>when the task’s outcome is expected farther in the future<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>,</li>
  <li>when they have low expectancy of successfully completing the task<sup id="fnref:3:1" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>, and</li>
  <li>when the task offers numerous junctures for decision-making (e.g., the student doesn’t know where to start)<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></li>
</ul>

<p>Can we use this knowledge to reduce the frequency of procrastination on these projects and ameliorate its negative effects? I previously wrote about the pervasive and pernicious nature of procrastination in this context—students start late, work late, and finish late, often resulting in degraded project outcomes.</p>

<p>In this post, I describe a simple classroom intervention that helped reduce the rate of late submissions on programming projects by about 30%, and helped improve class performance in the intermediate Data Structures and Algorithms (DSA) course at Virginia Tech.</p>

<h2 id="explicit-project-milestones">Explicit project milestones</h2>

<p>Our intermediate DSA course involves three to four programming projects, each of which is worked on for about a month. Each project requires students to build one or more interacting data structures—for example, a hash table or a PR quad-tree. Projects are un-scaffolded (no starter code) and students are free to design their own solutions. They are allowed to make as many submissions as they like to our auto-grader, which tests their solutions using acceptance tests written by the course staff.</p>

<p>Unfortunately, these projects usually have distressingly high rates of late submissions and failing grades, and about 25–30% of students either fail or withdraw from the course.</p>

<p>We believe that poor self-regulation and project management (often manifesting as procrastination) may be a significant contributor to these poor outcomes. Therefore, we instituted explicit project milestones—we broke down projects into sub-tasks that students needed to complete by intermediate deadlines. We expected to observe reduced procrastination stemming from</p>
<ul>
  <li>outcomes that are too far in the future to be valued in the present,</li>
  <li>students not being able to decompose a large engineering task into manageable sub-tasks, and</li>
  <li>low expectancy of success, because completing each successive sub-task is likely to increase the student’s self-efficacy regarding the larger overarching task</li>
</ul>

<p>The milestones themselves were pretty straightforward, and checkable using our auto-grading infrastructure. For any given data structure (or database made up of interacting data structures), they were as follows:</p>
<ul>
  <li><strong>Milestone 1</strong>: Make a first submission to the auto-grader (speaking as a former instructor for this course, this was surprisingly effective at making students start thinking about the project early)</li>
  <li><strong>Milestone 2</strong>: Complete the insertion operation</li>
  <li><strong>Milestone 3</strong>: Complete the search and update operations</li>
  <li><strong>Final submission</strong>: Complete the removal operation</li>
</ul>

<p>Done in that order, the removal operation is completed last. This is good because removal is often the hardest-to-implement operation in many data structures, involving complex structural rearrangements (e.g., re-ordering a binary search tree, or re-balancing a B+ tree). Our hope is that the experience of successfully completing the other, simpler operations a week or two earlier helps build the student’s self-efficacy regarding the more difficult sub-task ahead.</p>

<h2 id="how-helpful-were-the-milestones">How helpful were the milestones?</h2>
<p>We instituted Milestones in the Spring of 2016. We are interested in knowing if the milestones had any impact on procrastination, project outcomes, or course outcomes. So we compared outcomes in the “milestones semester” (Spring 2016) with outcomes in a “no milestones semester”—Fall 2013. In Fall 2013, the course was taught by the same instructor (Cliff), and the projects were of comparable difficulty. In terms of LoC and cyclomatic complexity, there were no differences between project submissions in the two semesters.</p>

<p>We measured differences in terms of timeliness, project correctness, and course outcomes. I present a high-level overview of our results below. More details can be found in the paper.</p>

<h3 id="timeliness">Timeliness</h3>

<p>Milestones were pretty effective! We measured the rate of late submissions and the time of project completion for the two semesters. We checked for differences between the two semesters as well as differences within the milestones semester based on the number of milestones completed.</p>
<ul>
  <li>The rate of late submissions dropped from 41% with milestones to 12% without milestones</li>
  <li>Within the treatment semester, students who completed 0 or 1 milestones tended to finish on the day of or after the deadline, while students who completed 2 or 3 milestones tended to finish their projects a day or more before the deadline</li>
</ul>

<p>Results are summarised in the figure below.</p>

<figure>
  <a href="/assets/posts/images/explicit-project-milestones/completion-time-milestone-group.png" target="_blank">
    
    <img src="/assets/posts/images/explicit-project-milestones/completion-time-milestone-group.png" alt="A series of boxplots showing distributions of project completion times in terms of hours before the deadline. They generally completing more milestones led to generally earlier completion times." />
    
  </a>

  <figcaption>Completion times (in terms of hours before or after the deadline) with and without milestones. Whiskers are the 10th and 90th percentiles. Outliers beyond these percentiles are omitted.</figcaption>
</figure>

<h3 id="project-correctness">Project correctness</h3>

<p>As I stated above, a lack of self-efficacy or discipline-specific procedural knowledge may be contributing to students’ poor outcomes on these projects. In addition to late submissions, students are prone to submitting incorrect or inadequately tested solutions. So we checked to see if milestones had any impact on project correctness.</p>

<p>Project correctness was defined as the percentage of instructor-written acceptance tests that the student’s submission passed. We checked for differences in correctness between submissions in the two semesters, and differences within the milestones semester based on the number of milestones completed.</p>

<ul>
  <li>The mean correctness score increased slightly, from 72% (sd = 20%) without milestones to 76% (sd = 21%) with milestones</li>
  <li>Students who completed 2 or more milestones tended to produce projects with higher correctness than those who completed fewer milestones</li>
</ul>

<figure>
  <a href="/assets/posts/images/explicit-project-milestones/correctness-milestone-group.png" target="_blank">
    
    <img src="/assets/posts/images/explicit-project-milestones/correctness-milestone-group.png" alt="A column chart showing pass, withdraw, and failure rates between the milestones term and the term without milestones. There were largely no changes." />
    
  </a>

  <figcaption>Correctness scores with and without milestones</figcaption>
</figure>

<h3 id="course-outcomes">Course outcomes</h3>
<p>Having seen that milestones had a positive impact on timeliness and correctness, we now turn to course outcomes.</p>

<p><strong>First</strong>, we examine the pass, fail, and withdrawal rates in the course.</p>

<p>Students at our institution tend to find this course challenging, and it is common to see 25–30% of students withdraw from the course or fail to achieve a grade that will let them progress on to subsequent courses.</p>

<p>Success in the course is largely driven by success on the projects, and students may be withdrawing from the course based on early bad outcomes. So did milestones help to brighten this gloomy picture?</p>

<p>Sadly, no.</p>

<p>We first checked for differences in the frequency of the following course outcomes:</p>
<ul>
  <li><em>Pass</em>, where the student achieved a grade that let them move on in the major</li>
  <li><em>Fail</em>, where the student completed the course, but would need to take it again before moving on</li>
  <li><em>Withdraw</em>, where the student did not complete the course<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></li>
</ul>

<!-- [<sup id='1-source'>1</sup>](#1-sink) -->

<p>We didn’t find any differences in these outcomes between the two semesters. This is apparent in the figure below.</p>

<figure>
  <a href="/assets/posts/images/explicit-project-milestones/outcomes.png" target="_blank">
    
    <img src="/assets/posts/images/explicit-project-milestones/outcomes.png" alt="" />
    
  </a>

  <figcaption>Milestones seem not to have affected course Pass, Fail, and Withdraw rates.</figcaption>
</figure>

<p><strong>Next</strong>, we explored the impact that milestones had on frequencies of final course grades, which were a composite of project performance, exam performance, and performance on mastery-based homework assignments. Differences in these frequencies could indicate the specific groups of students who were helped or not helped by the milestones intervention.</p>

<p>We examined differences in the frequencies of <em>A</em>, <em>B</em>, <em>C</em>, <em>D</em>, and <em>F</em> grades between the two semesters. Results are in the figure below.</p>

<figure>
  <a href="/assets/posts/images/explicit-project-milestones/grades.png" target="_blank">
    
    <img src="/assets/posts/images/explicit-project-milestones/grades.png" alt="A column chart showing percentages of A, B, C, D, and F grades in the two terms. There were more A grades and fewer B grades in the term with milestones than the term without. No changes in other grade frequencies are apparent." />
    
  </a>

  <figcaption>Final course grades with and without milestones.</figcaption>
</figure>

<p>More students received <em>A</em> grades in the milestones semester (54%) than in the semester without milestones (34%). In contrast, fewer students received <em>B</em> grades in the milestones semester (20%) than in the semester without milestones (38%). We found no other differences.</p>

<p>Taken together, the results above suggest that the positive effects of the milestones were more pronounced for the students “in the middle”: many <em>B</em>-level students became <em>A</em>-level students, but unfortunately the <em>C</em>-, <em>D</em>-, and <em>F</em>-level students continued to struggle in the course.</p>

<p>It seems as though this intervention, while helpful, failed to assist the most vulnerable students in the course. Other supports and instructional changes are needed to help those students.</p>

<h2 id="what-did-students-think-of-the-milestones">What did students think of the milestones?</h2>
<p>We included this question in an informal end-of-term survey:</p>

<p><em>How helpful did you find the Milestones in completing your programming projects on time? Please explain why you gave this response.</em></p>

<p>75% of students found the milestones to be Helpful or Very helpful. Students mentioned that milestones helped them avoid procrastination, encouraged them along the path to project completion, and helped them decompose the project.</p>

<p>15% of students were Neutral about the milestones. These students indicated that they were already on track without the milestones.</p>

<p>10% of students who found milestones to be Not helpful or Not at all helpful, and indicated that milestones were stressful and sometimes interfered with their existing project development plans.</p>

<p>We interpret the above to mean that perceptions of the milestones were largely positive, though some students found them to be unnecessary or stressful.</p>

<p>Note that this was just a single question in a survey — this is not a detailed qualitative analysis of students’ attitudes toward assigned project milestones.</p>

<blockquote>
  <p>Aside: I’ve used milestones when teaching this course over the Summer session, and it may have been a blunder in that context. Summers tend to be intense, squeezing 15 weeks of material into 6 weeks of daily instruction. Month-long projects are 2-week projects. It’s hard to space out milestones appropriately in such a short timeframe (avoiding weekend deadlines, etc.). I had a student mention that they worked a job during the week, and did coursework on weekends. Milestones made this difficult.</p>
</blockquote>

<h2 id="final-remarks">Final Remarks</h2>

<p>A possible criticism of this work is that the ability to decompose and tackle an un-scaffolded programming project is a core competency that students are expected to learn during intermediate programming courses, and they will no longer do so because “we did it for them”.</p>

<p>Fundamentally, the problem with this is that we don’t really teach these skills at this stage in the curriculum. So we end up putting students through a “trial-by-fire” in these courses, and those who make it can continue on in the major. Ironically, these are the students who reach the later software engineering and capstone courses in which we do teach these skills.</p>

<p>Milestones are by no means perfect, but we believe they help students (1) by making them practice a successful approach to large programming projects, and (2) by giving them the experience of successfully completing these projects, that they can then draw on in later courses. Effects of explicit milestones going into later courses is an open question.</p>

<p>I am sure that this is not a unique practice.
But I haven’t seen this at other institutions I’ve been affiliated with, and the literature review didn’t turn up anything in the context of intermediate project development. If your courses use milestones or something similar, how’s it going for you?</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://doi.org/10.1145/2591062.2591159">Radermacher, 2014</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.133.1.65">Steel, 2007</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><a href="http://doi.wiley.com/10.1002/j.2168-9830.2001.tb00599.x">Ponton, 2010</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><a href="http://ieeexplore.ieee.org/document/7017768/">Song, 2014</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Withdrawing is not always a bad thing; maybe they just didn’t want to learn the subject. But computing faces a retention crisis due a variety of external and internal factors, and a student in a third-year course probably did want to complete the course. So here, we treat it as a Bad Thing™. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="research-summary" /><category term="procrastination" /><summary type="html"><![CDATA[This article was originally posted on my Medium blog on October 14, 2020.]]></summary></entry><entry><title type="html">Assessing Incremental Testing Practices and Their Impact on Project Outcomes</title><link href="https://ayaankazerouni.org/posts/assessing-incremental-testing" rel="alternate" type="text/html" title="Assessing Incremental Testing Practices and Their Impact on Project Outcomes" /><published>2020-07-07T00:00:00-07:00</published><updated>2020-07-07T00:00:00-07:00</updated><id>https://ayaankazerouni.org/posts/assessing-incremental-testing</id><content type="html" xml:base="https://ayaankazerouni.org/posts/assessing-incremental-testing"><![CDATA[<p><small>
<em>This article was originally posted on <a href="https://ayaankazerouni.medium.com">my Medium blog</a> on July 7 2020.</em>
</small></p>

<p><em>This is a brief overview of the research paper “<strong><a href="/publications#sigcse2019paper”">Assessing Incremental Testing Practices and Their Impact on Project Outcomes</a></strong>, published at SIGCSE 2019. My co-authors were <a href="https://people.cs.vt.edu/~shaffer">Cliff Shaffer</a>, <a href="https://people.cs.vt.edu/~edwards">Steve Edwards</a>, and <a href="https://people.cs.vt.edu/~fservant">Francisco Servant</a>.</em></p>

<p>Software testing is the most common method of ensuring the correctness of software. As students work on relatively long-running software projects, we would like to know if they are engaging with testing consistently through the life-cycle. The long-term goal is to provide students with feedback about their testing habits as they work on projects.</p>

<p>This post is aimed at computing educators, researchers, and engineers.</p>

<p>We examined the programming effort applied by students during (unsupervised) programming sessions as they worked on projects, and measured the proportion of that effort that was devoted to writing tests. This measurement is useful because it lets us avoid the “test-first” or “test-last” dichotomy and allowed for varying styles and levels of engagement with testing over time. It can also be easily measured in real-time, facilitating automated and adaptive formative feedback.</p>

<h2 id="summary">Summary</h2>

<h3 id="goal">Goal</h3>
<ul>
  <li>To assess a student’s software testing, not just their software tests</li>
  <li>To understand how various levels of engagement with testing relate to project outcomes</li>
</ul>

<h3 id="method">Method</h3>
<ul>
  <li>Collect high-resolution project snapshot histories from consenting students’ IDEs and mine them for insight about their testing habits</li>
  <li>Test the relationships between students’ testing habits and their eventual project correctness and test suite quality</li>
</ul>

<h3 id="findings">Findings</h3>
<p>Unsurprisingly, we found that when more of each session’s programming effort was devoted to testing, students produced solutions with higher correctness and test suites with higher condition coverage. We also found that project correctness was unrelated to whether students did their testing before or after writing the relevant solution code.</p>

<p>Our findings suggest that the incremental nature of testing was more important than whether the student practiced “test-first” or “test-last” styles of development. The within-subjects nature of our experimental design hints that these relationships may be causal.</p>

<h2 id="motivation">Motivation</h2>
<p>Students and new software engineering graduates often display poor testing ability and a disinclination to practice regular testing.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> It is now common practice to require unit tests to be submitted along with project solutions. But it is unclear if students are writing these tests incrementally as they work toward a solution, or if they are following the less desirable “code a lot, test a little” style of development.</p>

<p>Learning a skill like incremental testing requires practice, and practice should be accompanied by feedback to maximise skill acquisition. But we cannot produce feedback about practice without observing it. We conducted a study in our third-year Data Structures &amp; Algorithms course to (1) measure students’ adherence to incremental test writing as they worked on large, complex programming projects, and (2) understand the impact that their test writing practices had on project outcomes.</p>

<p>We focused on addressing two challenges to the pedagogy of software testing:
First, existing testing feedback tends to focus on product rather than process. That is, assessments tend to be driven by “post-mortem” measures like code coverage, all-pairs execution, or (rarely) mutation coverage achieved by students’ submissions. Students’ adherence to test-oriented development processes as they produce those submissions is largely ignored.</p>

<p>We addressed this by measuring the balance of students’ test writing vs. solution writing activities as they worked on their projects. If we can reliably measure students’ engagement with testing as they work on projects, we can provide formative feedback to help keep them on track in the short term, and help them form incremental testing habits in the long term.</p>

<p>Second, there is a lack of agreement on what constitutes effective testing process. Numerous researchers have presented (often conflicting<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>) evidence about the effectiveness of test-driven development (TDD) and incremental test-last (ITL) styles of development. But these findings may not generalise to students learning testing practices, and the conflicting evidence muddies the issue of what exactly we should teach or prescribe.
We addressed this by avoiding the “TDD or not” dichotomy. Students’ (and indeed, professionals’) engagement with testing does not stay consistent over time, either in kind or in volume <sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup> <sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>. Therefore, we didn’t attempt to classify student work as following TDD or not. Instead, we measured the extent to which they balanced their test writing and solution writing activities for increments of work over the course of their projects. This allowed us to more faithfully represent and study their varying levels of engagement with testing as they worked toward solutions.</p>

<h2 id="measuring-incremental-test-writing">Measuring Incremental Test Writing</h2>

<h3 id="context-and-data-collection">Context and data collection</h3>
<p>Students in our context are working on software projects that are larger and more complex than what they have encountered in previous courses. Success usually necessitates adherence to disciplined software process, including time management and regular testing. We studied roughly 150 students working on 4 programming assignments over a semester (~3.5 month period).</p>

<p>We used an <a href="https://github.com/web-cat/eclipse-plugins-importer-exporter/tree/DevEventTrackerAddition">Eclipse plugin</a> to automatically collect frequent snapshots of students’ projects as they worked toward solutions. This snapshot history allowed us to paint a rich picture of a project’s evolution. In particular, it let us look at how the test code and solution code for a project emerged over time.</p>

<h3 id="proposed-measurements-of-testing-effort">Proposed measurements of testing effort</h3>
<p>Using these data, we measured the balance and sequence of students’ test writing effort with respect to their solution writing effort. I describe these measurements below.</p>

<p>Consider the following figure, which shows an example sequence of developer activity created from synthetic data. Colours indicate methods in the project; solid and shaded blocks are solution and test code, respectively; and groups of blocks indicate work sessions.</p>

<figure class="wide">
  <a href="/assets/posts/images/assessing-incremental-testing/example-dev-activity.png" target="_blank">
    <img src="/assets/posts/images/assessing-incremental-testing/example-dev-activity.png" alt="Coloured bars organised into groups, demarcating time spent writing production code and test code, devoted to individual methods." />
  </a>

  <figcaption>An example sequence of developer activity (synthetic data).</figcaption>
</figure>

<p>We can derive measurements of balance and sequence of testing with this synthetic data in mind.
The measures are summarised in the next figure.</p>

<p>In terms of <strong>balance</strong>, we considered testing effort in terms of <em>space</em> (methods in the project), <em>time</em> (work sessions), <em>both</em>, or <em>neither</em>.</p>

<ul>
  <li><strong>Neither</strong>: how much total testing took place over the course of the project and its lifecycle? (POB in the figure below)</li>
  <li><strong>Space only</strong>: how much testing took place for each method in the project? (MOB)</li>
  <li><strong>Time only</strong>: how much testing took place during each work session? (PSB)</li>
  <li><strong>Both time and space</strong>: how much testing took place for each method during each work session? (MSB)</li>
</ul>

<p>Each measurement is a proportion, i.e., the proportion of all code — written in a work session or for a method or on the entire project — that was test code. So if a student wrote 100 LoC in a work session, and 20 of them were test code, the proportion for that work session would be 0.2.</p>

<p>In terms of <strong>sequence</strong>, we measured the extent to which students tended to write test code before finalising the solution code under test (MOS below).<br />
This was measured as the proportion of test code devoted to a method that was written before the method was finalised (i.e., before it was changed for the last time).</p>

<p>Note that this is different from measuring adherence to “TDD or not”.
Instead of classifying a student’s work (in a work session, or on a method, etc.) as strictly test-first or test-last, we measure this concept on a continuous scale.
This allows a more nuanced discussion of students’ tendencies to write test code earlier or later with respect to the solution code that is being tested.</p>

<p>Measurements are depicted in the figure below. All measurements were aggregated as medians for each project.</p>

<figure>
  <a href="/assets/posts/images/assessing-incremental-testing/metrics.png" target="_blank">
    
    <img src="/assets/posts/images/assessing-incremental-testing/metrics.png" alt="Coloured bars depicting time spent writing and testing individual methods in a program. They are grouped in four different ways: grouped by time (ignoring the methods being focused on), grouped by method, grouped by method and time, and grouped by sequence." />
    
  </a>

  <figcaption>Measurements to be derived from a programming activity stream. Each row depicts a different way of aggregating the events from the figure above.</figcaption>
</figure>

<h2 id="findings-1">Findings</h2>

<p>With these measurements in hand, we are able to examine students’ incremental testing practices and their impacts on project outcomes.</p>

<h3 id="to-what-extent-did-students-practice-incremental-testing">To what extent did students practice incremental testing?</h3>

<p>Using the measurements we described, we can make observations about how students distributed their testing effort as they worked on projects.</p>

<p>The measurements are summarised in the figure below. The first four box-plots (in blue) represent measures of the balance of test writing effort. The fifth box-plot (in orange) represents proportion of test writing effort devoted before the relevant solution code was finalised.</p>

<figure>
  <a href="/assets/posts/images/assessing-incremental-testing/boxplots-balance-sequence.png" target="_blank">
    
    <img src="/assets/posts/images/assessing-incremental-testing/boxplots-balance-sequence.png" alt="" />
    
  </a>

  <figcaption>Distributions of testing effort measurements described above.</figcaption>
</figure>

<p>Our findings were as follows.</p>

<p><strong>Balance of testing effort</strong></p>

<ul>
  <li>Students tended to devote about 25% of their total code writing effort to writing test code <em>(POB)</em></li>
  <li>Within individual work sessions, a majority of students devoted less than 20% of their coding effort to writing tests <em>(PSB)</em></li>
  <li>The median method seemed to received a considerable amount of testing effort (<em>MOB</em>, <em>MSB</em>), possibly attributable to the way we tied solution methods and test methods together <sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote" rel="footnote">6</a></sup></li>
</ul>

<p>The lower proportions of testing effort observed per work session, relative to the total project testing effort, suggests that some sessions tend to see more testing effort than others. Prior work suggests that this increased testing tends to take place toward the end of the project life-cycle, to satisfy condition coverage requirements imposed on students as part of the programming assignments.</p>

<p>The lower proportions of testing effort observed per work session, relative to the total project testing effort, suggests that some sessions tend to see more testing effort than others. <a href="https://ayaankazerouni.github.io/assets/publications/quantifying-incremental-development-procrastination.pdf">Prior work</a> suggests that this increased testing tends to take place toward the end of the project life-cycle, to satisfy condition coverage requirements imposed on students as part of the programming assignments.</p>

<p><strong>Sequence of testing effort</strong></p>

<ul>
  <li>In an overwhelming majority of submissions (85%), students tended to do their testing after the relevant solution methods were finalised (<em>MOS</em>)</li>
</ul>

<h3 id="how-did-project-outcomes-relate-to-the-balance-and-sequence-of-test-writing-activities">How did project outcomes relate to the balance and sequence of test writing activities?</h3>

<p>We measured the relationship between the measurements described above and two outcomes of interest:</p>

<ul>
  <li><em>Correctness</em>, measured by an instructor-written oracle of reference tests</li>
  <li><em>Code coverage</em>, measured as the condition coverage achieved by the student’s own test suite</li>
</ul>

<p>We used linear mixed-effects models, with the outcome of interest as a dependent variable and the testing measurements as the independent variables.
This allowed us to tease out the variance in project outcomes that was explained by traits inherent to individual students.</p>

<p>We found that:</p>
<ul>
  <li>Students produced higher quality solutions and tests when they devoted a higher proportion of their programming effort in each session to writing tests</li>
  <li>Whether this testing occurred before or after the relevant solution code was written was irrelevant to project correctness</li>
</ul>

<p>We also found a <em>negative</em> relationship between doing more testing before finalising solution code and condition coverage scores.
We do not think this means that <em>testing first is bad</em>—more likely this is an effect of students adding tests to nearly complete projects to drive up condition coverage, which made up part of their grade.
Incentives beget behaviours!</p>

<p>Note that, after teasing out the variance explained by inherent variability in the students (conditional \(R^2\) in the paper), our measurements explained an ultimately small percentage of variance in project correctness and condition coverage (marginal \(R^2\) in the paper).
More variance in outcomes can possibly be explained by any number of unaccounted-for factors.
It is also possible that projects in the Data Structures course we studied didn’t “hit the right switches” in terms of depending on regular testing for success.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Our findings largely support the conventional wisdom about the virtues of regular software testing.
We did not find support for the notion that writing tests first leads to better project outcomes.</p>

<p>Traits or situations inherent to individual students are unlikely to have affected our results.
Students’ differing behaviours <em>and</em> outcomes could both be symptoms of some other unknown factors (e.g., prior programming experience, differing demands on time).
Therefore, we used <em>within-subjects</em> comparisons—i.e., assignments served as repeated measures for each student.
Each student’s work on a given project was compared to their own work on other projects, and differences in testing practices and project outcomes were observed.</p>

<p>The primary contribution of this work is that we are able to measure a student’s adherence to these practices with some lead time before final project deadlines.
The short-term benefit of this is that we can provide feedback to students “before the damage is done”, i.e., before they face the consequences of poor testing practices that we have measured and observed.</p>

<p>In the long-term, we think that formative feedback about testing, delivered as students work on projects, can help them to form disciplined test-oriented development habits.</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://dl.acm.org/doi/10.1145/2445196.2445351">Radermacher, 2013</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://ieeexplore.ieee.org/abstract/document/7592412?casa_token=EairhmqPPOgAAAAA:ag2Z44va8LgKn-K5NhAhDLFyn-nmTRksrVGQTMvB2qftTlfS94O1FZZyClPDUvT8qM011tfC_ls">Fucci, 2017</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><a href="https://dl.acm.org/doi/10.1145/1159733.1159787">Bhat, 2006</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><a href="https://inventitech.com/assets/publications/2017_beller_gousios_panichella_amann_proksch_zaidman_developer_testing_in_the_ide_patterns_beliefs_and_behavior.pdf">Beller, 2015</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p><a href="https://inventitech.com/assets/publications/2017_beller_gousios_panichella_amann_proksch_zaidman_developer_testing_in_the_ide_patterns_beliefs_and_behavior.pdf">Beller, 2015a</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Solution methods were tied to test methods if they were directly invoked in the test method. So whether or not an invoked method was the “focal method” of the test, it was treated as “being tested”. This could have inflated results for methods that were commonly used to set up test cases. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="research-summary" /><category term="software-testing" /><summary type="html"><![CDATA[This article was originally posted on my Medium blog on July 7 2020.]]></summary></entry><entry><title type="html">Developing Procrastination Feedback for Student Software Developers</title><link href="https://ayaankazerouni.org/posts/procrastination-feedback" rel="alternate" type="text/html" title="Developing Procrastination Feedback for Student Software Developers" /><published>2020-04-17T00:00:00-07:00</published><updated>2020-04-17T00:00:00-07:00</updated><id>https://ayaankazerouni.org/posts/procrastination-feedback</id><content type="html" xml:base="https://ayaankazerouni.org/posts/procrastination-feedback"><![CDATA[<p><small>
<em>This article originally appeared on <a href="https://ayaankazerouni.medium.com">my Medium blog</a> on April 17, 2020.</em>
</small></p>

<p><em>This is a brief overview of the following research papers by myself, <a href="https://people.cs.vt.edu/~edwards/">Steve Edwards</a>, and <a href="https://people.cs.vt.edu/~shaffer/">Cliff Shaffer</a>:</em></p>

<ul>
  <li><em><strong><a href="/publications/iticse2017paper">DevEventTracker: Tracking Development Events to Assess Incremental Development and Procrastination</a></strong></em> (with T. Simin Hall), published at ITiCSE 2017, and</li>
  <li><em><strong><a href="/publications/icer2017paper">Quantifying Incremental Development Practices and Their Relationship to Procrastination</a></strong></em>, published at ICER 2017</li>
</ul>

<p>I am summarising these papers together because they are closely related.</p>

<h2 id="summary">Summary</h2>

<p>We would like to determine the effectiveness of the time management practices displayed by students as they work on large and complex programming projects for the first time. We used qualitative data (obtained from interviews with students) and quantitative data (obtained using IDE logging infrastructure) to characterise their software development habits, and we analysed their relationships with project outcomes like correctness, total time taken, and the project’s early or late status.</p>

<p>When students worked earlier and more often, they produced projects that:</p>
<ul>
  <li>were more correct,</li>
  <li>were completed earlier,</li>
  <li>took no more or less time to complete</li>
</ul>

<p>So working earlier and more often doesn’t seem to be giving the student more time to complete projects, just more <em>constructive</em> time.</p>

<h2 id="motivation">Motivation</h2>

<p>Software development is a skill. Like any skill, it requires practice and feedback in order to develop. Ideally, this feedback should formative — delivered as students work on projects. However, in education contexts, assessments of software projects are driven by “after-the-fact” qualities like correctness, code coverage, code style, etc. In the papers listed above, my co-authors and I present methods to characterise students’ time management habits as they work on large and complex projects. The goal is to use this information to formulate formative feedback about their development practices.</p>

<h2 id="observing-the-development-process">Observing the development process</h2>

<p>To properly assess a ~30-hour programming process, we need to be able to observe it. We developed an Eclipse plugin that emits events for various in-IDE actions, including:</p>
<ul>
  <li>executions</li>
  <li>compilations</li>
  <li>file saves</li>
  <li>line-level edits</li>
</ul>

<p>We use these data to capture, characterise, and determine the effectiveness of the software development process undertaken by students. This process involved ingesting a large (ish) volume of data and turning it into an objective measurement of some aspect of the programming process (in this case, procrastination).</p>

<h2 id="when-do-students-work-on-software-projects">When do students work on software projects?</h2>

<p>We set out to quantitatively measure the extent to which <strong>procrastination</strong> manifests as students work on software projects. We look at the work done by students as a distribution of work days, from the first day the student worked on the project until the last day, typically the day of the project deadline. The value for each work day is the amount of observable “work” that was put in on the project — the number of character code edits. The mean of this distribution gives us the “average day” on which students tended to work on the project. If we measure this in terms of <em>days until the deadline</em>, then a higher number indicates that more work was done earlier, and a lower number indicates that more work was done closer to the deadline.</p>

<p>As an example, consider the figure below, which shows how a real student distributed their work across the days on which they worked on a project.</p>

<figure>
  <a href="/assets/posts/images/procrastination-feedback/tm-early-often.png" target="_blank">
    
    <img src="/assets/posts/images/procrastination-feedback/tm-early-often.png" alt="" width="400" />
    
  </a>

  <figcaption>The mean edit time for a student, drawn from real data.</figcaption>
</figure>

<!-- A bar chart showing the amount of work put in by a student on each day from August 28 to September 14. -->
<p>The red line on September 14 indicates the project deadline, and the black line on September 8 indicates the student’s “mean edit time”, which is 6 days before the deadline. A sizeable portion of work was done within the period of September 1 to September 8, and daily work was much higher during the last three days of the project lifecycle. This leads the mean edit time to be roughly in the middle of those time periods. The student’s score is therefore sensitive to not only the days on which was done, but also to the amount of work that was done on those days. Since this is simply a mean edit time, we can measure this with solution code, test code, or both.</p>

<p>We might also have measured the median edit time (i.e., on what day was half the work done on a project?). However, we opted for the mean since it is more sensitive to outliers, which are important when measuring procrastination (e.g., large amounts of code being written toward the end of a project timeline).</p>

<p>The figure below indicates distributions of the mean edit time for solution code and for test code, across all project implementations.</p>

<figure>
  <a href="/assets/posts/images/procrastination-feedback/solution-test-early-often-dists.png" target="_blank">
    
    <img src="/assets/posts/images/procrastination-feedback/solution-test-early-often-dists.png" alt="Two box-and-whisker plots which show the mean edit times for solution code and test code." width="400" />
    
  </a>

  <figcaption>On average, students tended to write code fewer than 10 days before the project deadline.</figcaption>
</figure>

<!-- On average, students tended to write code fewer than 10 days before the project deadline. -->
<p>This figure tells us that students tended to work rather close to the deadline, even though they were given about 30 days to work on projects. Similar distributions of mean times were observed for solution code (\(\mu=8.48, \sigma=6.44\)), test code (\(\mu=7.78, \sigma=7.04\)), program executions (\(\mu=8.86, \sigma=8.82\)), and test executions (\(\mu=7.09, \sigma=7.10\)). Test editing and launching tends to occur slightly closer to the project deadline, but this difference appears to be negligible.</p>

<h2 id="how-valid-is-our-measurement">How valid is our measurement?</h2>

<p>The measurement described above is simple enough: it’s just a mean. Still, it is worth investigating whether it measures what we think it measures, i.e., the extent to which procrastination manifests on a software project. There is no readily-available “ground truth” against which one can test such a measurement. Therefore, we interviewed students in depth about their development experiences on two such assignments, and compared their responses with our measurements. Interviewees were given our measurements at the end of the interview, and we determined if they matched students’ expectations.</p>

<p>In general, students felt that our measurements were accurate. Additionally, students believed that feedback driven by a measure such as this could help them stay on track on future programming projects. They stated unconditionally that they would make more of an effort to improve their programming practice if they were given feedback about their process between assignments.</p>

<h2 id="can-this-measurement-explain-differences-in-project-outcomes">Can this measurement explain differences in project outcomes?</h2>

<p>A primary thesis of these papers was that different software development habits can explain differences in software project outcomes for intermediate-to-advanced student software developers. With our measures and their qualitative evaluations in hand, we set out to quantitatively examine their relationships with the following project outcomes:</p>
<ul>
  <li><strong>Project correctness</strong>, measured as the percentage of instructor-written reference tests passed,</li>
  <li><strong>Time of completion</strong>, measured as the number of hours before the deadline the project was completed, and</li>
  <li><strong>Total time spent</strong>, measured by adding up the lengths of all work sessions spent on the project</li>
</ul>

<p>We used within-subjects comparisons to make inferences, allowing us to control for traits unique to individual students. Different students’ behaviours <em>and</em> outcomes could be symptoms of some other unknown factor (e.g., differing course loads or prior experience), making such inferences weaker. To test for relationships with the outcome variables, we used an ANCOVA with repeated measures for each student. Students were subjects, and assignments served as repeated measures (with unequal variances), allowing within-subjects comparisons. In other words, each student’s software development habits were measured repeatedly (assignments), and differences in outcomes for the same student were analysed.</p>

<p>Results are summarised below.</p>

<p><strong>When students worked earlier and more often, they tended to produce programs with higher correctness.</strong> To illustrate this, we split the dataset roughly into half: those projects that had “solved” the assigned problem (53%), and those that had not (47%). The figure below shows the difference in edit mean times between these populations.</p>

<figure>
  <a href="/assets/posts/images/procrastination-feedback/solution-early-often-by-solved.png" target="_blank">
    
    <img src="/assets/posts/images/procrastination-feedback/solution-early-often-by-solved.png" alt="Two box-and-whisker plots showing the mean edit time for solved and unsolved projects." width="500" />
    
  </a>

  <figcaption>Comparison of solution edit times between projects that correctly solved an assignment, and those that did not.</figcaption>
</figure>

<p><strong>When students worked earlier and more often, they tended to finish their projects earlier.</strong> This is so intuitive, it’s almost tautological. It is encouraging that the measurement is able to discriminate between early and late project submissions.</p>

<figure>
  <a href="/assets/posts/images/procrastination-feedback/solution-early-often-by-on-time-status.png" target="_blank">
    
    <img src="/assets/posts/images/procrastination-feedback/solution-early-often-by-on-time-status.png" alt="Two box-and-whisker plots showing the mean edit time for late and on-time submissions." width="500" />
    
  </a>

  <figcaption>Comparison of solution edit times between on-time and late submissions.</figcaption>
</figure>

<p>Finally, <strong>there was no relationship between total amount of time spent on the project and the solution edit mean time.</strong></p>

<h2 id="final-remarks">Final Remarks</h2>

<p>The important takeaway from these papers is not the revelation that <em>procrastination = bad</em>. It is that we can reliably identify when procrastination is taking place on software projects. If we can do this during a project timeline, i.e., while the student is working on it, we may be able to intervene and help them adjust their programming behaviours before they face the consequences of procrastination.</p>]]></content><author><name></name></author><category term="research-summary" /><category term="procrastination" /><summary type="html"><![CDATA[This article originally appeared on my Medium blog on April 17, 2020.]]></summary></entry></feed>