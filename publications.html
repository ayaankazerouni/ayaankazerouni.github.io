<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta http-equiv="x-ua-compatible" content="ie=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<title>Publications - Ayaan M. Kazerouni</title>

	<link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="/assets/academicons/css/academicons.min.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.1/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
	<link rel="canonical" href="ayaankazerouni.github.io/publications">
	<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,300,700,800,600' rel='stylesheet' type='text/css'>
	<link href="https://fonts.googleapis.com/css2?family=EB+Garamond:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
	<link href='https://fonts.googleapis.com/css?family=Muli:400,300' rel='stylesheet' type='text/css'>
	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />

</head>


<body>
	
	<aside>
    <nav>
        <ul>
        
            
            <li><a href="/#" class="nav-link">Home</a></li>
        
            
            <li><a href="/teaching" class="nav-link">Teaching</a></li>
        
            
            <li><a href="/projects" class="nav-link">Projects</a></li>
        
            
            <li><a href="/publications" class="nav-link active">Publications</a></li>
        
            
            <li><a href="/about" class="nav-link">About Me</a></li>
        
        </ul>
    </nav>
</aside>


<header class='pages'>
  <div class='info'>
    <h1><a href=>Ayaan M. Kazerouni</a></h1>
    <strong>Asst. Professor, CS @ Cal Poly </strong><br>
    <a href="mailto:ayaan@vt.edu" class='address'>ayaan@vt.edu</a>
  </div>
</header>


<!-- <header> -->
  <!-- <h1><a href="/">Ayaan M. Kazerouni</a></h1>
  <div></div><a href="mailto:ayaan@vt.edu">ayaan@vt.edu</a><br></div>
  <strong>Asst. Professor of Computer Science</strong> -->
  <!-- <div class='contact'>
    <a href='https://twitter.com/ayaankazerouni'>
      <i class='fab fa-twitter-square' aria-label="Twitter" title="Twitter"></i>
      <span class='visible-hidden'>Twitter profile</span>
    </a>
    <a href='https://github.com/ayaankazerouni'>
      <i class='fab fa-github' aria-label="GitHub", title="GitHub"></i>
      <span class='visible-hidden'>GitHub profile</span>
    </a>
    <a href='https://www.linkedin.com/in/ayaankazerouni'>
      <i class='fab fa-linkedin' aria-label="LinkedIn" title="LinkedIn"></i>
      <span class='visible-hidden'>LinkedIn profile</span>
    </a>
    <a href='https://scholar.google.com/citations?user=jGSnCrUAAAAJ&hl=en'>
      <i class='ai ai-google-scholar-square' aria-label="Google Scholar" title="Google Scholar"></i>
      <span class='visible-hidden'>Google Scholar profile</span>
    </a>
    <a href='https://orcid.org/0000-0002-6574-1278'>
      <i class='ai ai-orcid-square' aria-label="OrcID" title="OrcID"></i>
      <span class='visible-hidden'>OrcID profile</span>
    </a>
  </div> -->
<!-- </header> -->

	
	<main>
		<article>
			<h1>Publications</h1>

Papers and posters I have authored or co-authored. Click on items to see their abstracts.
<div class='content-box'>
<h2 class='title'>
Conference papers
</h2>
<div class='content'>
<div class='artifact-list'>
<details class='artifact'>
<summary>
ProgSnap2: A Flexible Format for Programming Process Data
<b>
(ITiCSE '20)
</b>
<div class='sub'>
T. Price, D. Hovemeyer, K. Rivers, A. C. Bart, G. Gao, <b>A. M. Kazerouni</b>, B. Becker, A. Petersen, L. Gusukuma, S. H. Edwards, D. Babcock
<br>
<a class='artifact-link' href='assets/publications/iticse20-progsnap2.pdf'>pdf</a>
<a class='artifact-link' href='https://doi.org/10.1145/3341525.3387373'>link</a>
</div>
</summary>
<p>
<b>Abstract:</b>
In this paper, we introduce ProgSnap2, a standardized format for logging programming process data. ProgSnap2 is a tool for computing education researchers, with the goal of enabling collaboration by helping them to collect and share data, analysis code, and data-driven tools to support students. We give an overview of the format, including how events, event attributes, metadata, code snapshots and external resources are represented. We also present a case study to evaluate how ProgSnap2 can facilitate collaborative research. We investigated three metrics designed to quantify students' difficulty with compiler errors - the Error Quotient, Repeated Error Density and Watwin score - and compared their distributions and ability to predict students' performance. We analyzed five different ProgSnap2 datasets, spanning a variety of contexts and programming languages. We found that each error metric is mildly predictive of students' performance. We reflect on how the common data format allowed us to more easily investigate our research questions.
</p>
</details>
<details class='artifact'>
<summary>
Testing Regex Generalizability And Its Implications: A Large-Scale Many-Language Measurement Study
<b>
(ASE '19)
</b>
<div class='sub'>
J. C. Davis, D. Moyer, <b>A. M. Kazerouni</b>, D. Lee
<br>
<a class='artifact-link' href='assets/publications/regex-generalizability-2019.pdf'>pdf</a>
<a class='artifact-link' href='https://doi.org/10.1109/ASE.2019.00048'>link</a>
</div>
</summary>
<p>
<b>Abstract:</b>
The regular expression (regex) practices of software engineers affect the maintainability, correctness, and security of their software applications. Empirical research has described characteristics like the distribution of regex feature usage, the structural complexity of regexes, and worst-case regex match behaviors. But researchers have not critically examined the methodology they follow to extract regexes, and findings to date are typically generalized from regexes written in only 1-2 programming languages. This is an incomplete foundation. Generalizing existing research depends on validating two hypotheses: (1) Various regex extraction methodologies yield similar results, and (2) Regex characteristics are similar across programming languages. To test these hypotheses, we defined eight regex metrics to capture the dimensions of regex representation, string language diversity, and worst-case match complexity. We report that the two competing regex extraction methodologies yield comparable corpuses, suggesting that simpler regex extraction techniques will still yield sound corpuses. But in comparing regexes across programming languages, we found significant differences in some characteristics by programming language. Our findings have bearing on future empirical methodology, as the programming language should be considered, and generalizability will not be assured. Our measurements on a corpus of 537,806 regexes can guide data-driven designs of a new generation of regex tools and regex engines.
</p>
</details>
<details class='artifact'>
<summary>
The Relationship Between Practicing Short Programming Exercises and Exam Performance
<b>
(CompEd '19)
</b>
<div class='sub'>
S. H. Edwards, K. P. Murali, <b>A. M. Kazerouni</b>
<br>
<a class='artifact-link' href='assets/publications/codeworkout-comped-2019.pdf'>pdf</a>
<a class='artifact-link' href='https://doi.org/10.1145/3300115.3309525'>link</a>
</div>
</summary>
<p>
<b>Abstract:</b>
Learning to program can be challenging. Many instructors use drill-and-practice strategies to help students develop basic programming techniques and improve their confidence. Online systems that provide short programming exercises with immediate, automated feedback are seeing more frequent use in this regard. However, the relationship between practicing with short programming exercises and performance on larger programming assignments or exams are unclear. This paper describes an evaluation of short programming questions in the context of a CS1 course where they were used on both homework assignments, for practice and learning, and on exams, for assessing individual performance. The open-source drill-and-practice system used here provides for full feedback during practice exercises. During exams, it allows limiting feedback to compiler errors and to a very small number of example inputs shown in the question, instead of the more complete feedback received during practice. Using data collected from 200 students in a CS1 course, we examine the relationship between voluntary practice on short exercises and subsequent performance on exams, while using an early exam as a control for individual differences including ability level. Results indicate that, after controlling for ability, voluntary practice does contribute to improved performance on exams, but that motivation to improve may also be important.
</p>
</details>
<details class='artifact'>
<summary>
Assessing Incremental Testing Practices and Their Impact on Project Outcomes
<b>
(SIGCSE '19)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>, C. A. Shaffer, S. H. Edwards, F. Servant
<br>
<a class='artifact-link' href='assets/publications/assessing-incremental-testing.pdf'>pdf</a>
<a class='artifact-link' href='assets/slides/assessing-incremental-testing-slides.pdf'>slides</a>
<a class='artifact-link' href='https://doi.org/10.1145/3287324.3287366'>link</a>
<a class='artifact-link' href='https://github.com/ayaankazerouni/incremental-testing'>code</a>
<div class='award artifact-link'>
2nd Best Research Paper
</div>
</div>
</summary>
<p>
<b>Abstract:</b>
Software testing is an important aspect of the development process, one that has proven to be a challenge to formally introduce into the typical undergraduate CS curriculum. Unfortunately, existing assessment of testing in student software projects tends to focus on evaluation of metrics like code coverage over the finished software product, thus eliminating the possibility of giving students early feedback as they work on the project. Furthermore, assessing and teaching the process of writing and executing software tests is also important, as shown by the multiple variants proposed and disseminated by the software engineering community, e.g., test-driven development (TDD) or incremental test-last (ITL). We present a family of novel metrics for assessment of testing practices for increments of software development work, thus allowing early feedback before the software project is finished. Our metrics measure the balance and sequence of effort spent writing software tests in a work increment. We performed an empirical study using our metrics to evaluate the test-writing practices of 157 advanced undergraduate students, and their relationships with project outcomes over multiple projects for a whole semester. We found that projects where more testing effort was spent per work session tended to be more semantically correct and have higher code coverage. The percentage of method-specific testing effort spent before production code did not contribute to semantic correctness, and had a negative relationship with code coverage. These novel metrics will enable educators to give students early, incremental feedback about their testing practices as they work on their software projects.
</p>
</details>
<details class='artifact'>
<summary>
Quantifying Incremental Development Practices and Their Relationship to Procrastination
<b>
(ICER '17)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>, S. H. Edwards, C. A. Shaffer
<br>
<a class='artifact-link' href='assets/publications/quantifying-incremental-development-procrastination.pdf'>pdf</a>
<a class='artifact-link' href='https://doi.org/10.1145/3105726.3106180'>link</a>
<a class='artifact-link' href='https://github.com/ayaankazerouni/sensordata'>code</a>
<a class='artifact-link' href='https://medium.com/@ayaankazerouni/developing-procrastination-feedback-for-student-software-developers-1652de60db7f'>blog</a>
</div>
</summary>
<p>
<b>Abstract:</b>
We present quantitative analyses performed on character-level program edit and execution data, collected in a junior-level data structures and algorithms course. The goal of this research is to determine whether proposed measures of student behaviors such as incremental development and procrastination during their program development process are significantly related to the correctness of final solutions, the time when work is completed, or the total time spent working on a solution. A dataset of 6.3 million fine-grained events collected from each student's local Eclipse environment is analyzed, including the edits made and events such as running the program or executing software tests. We examine four primary metrics proposed as part of previous work, and also examine variants and refinements that may be more effective. We quantify behaviors such as working early and often, frequency of program and test executions, and incremental writing of software tests. Projects where the author had an earlier mean time of edits were more likely to submit their projects earlier and to earn higher scores for correctness. Similarly earlier median time of edits to software tests was also associated with higher correctness scores. No significant relationships were found with incremental test writing or incremental checking of work using either interactive program launches or running of software tests, contrary to expectations. A preliminary prediction model with 69% accuracy suggests that the underlying metrics may support early prediction of student success on projects. Such metrics also can be used to give targeted feedback to help students improve their development practices.
</p>
</details>
<details class='artifact'>
<summary>
DevEventTracker: Tracking development events to assess incremental development and procrastination
<b>
(ITiCSE '17)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>, S. H. Edwards, T. S. Hall, C. A. Shaffer
<br>
<a class='artifact-link' href='assets/publications/deveventtracker.pdf'>pdf</a>
<a class='artifact-link' href='https://doi.org/10.1145/3059009.3059050'>link</a>
<a class='artifact-link' href='https://github.com/web-cat/eclipse-plugins-importer-exporter/tree/DevEventTrackerAddition'>code</a>
<a class='artifact-link' href='https://medium.com/@ayaankazerouni/developing-procrastination-feedback-for-student-software-developers-1652de60db7f'>blog</a>
</div>
</summary>
<p>
<b>Abstract:</b>
Good project management practices are hard to teach, and hard for novices to learn. Procrastination and bad project management practice occur frequently, and may interfere with successfully completing major programming projects in mid-level programming courses. Students often see these as abstract concepts that do not need to be actively applied in practice. Changing student behavior requires changing how this material is taught, and more importantly, changing how learning and practice are assessed. To provide proper assessment, we need to collect detailed data about how each student conducts their project development as they work on solutions. We present DevEventTracker, a system that continuously collects data from the Eclipse IDE as students program, giving us in-depth insight into students' programming habits. We report on data collected using DevEventTracker over the course of four programming projects involving 370 students in five sections of a Data Structures and Algorithms course over two semesters. These data support a new measure for how well students apply "incremental development" practices. We present a detailed description of the system, our methodology, and an initial evaluation of our ability to accurately assess incremental development on the part of the students. The goal is to help students improve their programming habits, with an emphasis on incremental development and time management.
</p>
</details>
</div>
</div>
</div>
<div class='content-box'>
<h2 class='title'>Posters and extended abstracts</h2>
<div class='content'>
<div class='artifact-list'>
<details class='artifact'>
<summary>
Toward Continuous Assessment of the Programming Process &mdash; Doctoral Consortium
<b>
(ICER '19)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>
<br>
<a href='assets/publications/icer-dc.pdf'>pdf</a>
<a href='https://doi.org/10.1145/3291279.3339429'>link</a>
</div>
</summary>
<p>
<b>Abstract:</b>
Assessment of software tends to focus on postmortem evaluation of metrics like correctness, mergeability, and code coverage. This is evidenced in the current practices of continuous integration and deployment that focus on software's ability to pass unit tests before it can be merged into a deployment pipeline. However, little attention or tooling is given to the assessment of the software development process itself. Good process becomes both more challenging and more critical as software complexity increases. Real-time evaluation and feedback about a student's software development skills, such as incremental development, testing, and time management, could greatly increase productivity and improve the ability to write tested and correct code. In my research, I develop models to quantify a student's programming process in terms of these metrics. By measuring the programming process, I can empirically evaluate its adherence to known best practices in software engineering. With the ability to characterize this, I can build tools to provide them with intelligent and timely feedback when they are in danger of straying from those practices. In the long term, I hope to contribute to the standardization and adoption of continuous software assessment techniques that include not only the final product, but also the process undertaken to produce it.
</p>
</details>
<details class='artifact'>
<summary>
Student Debugging Practices and Their Relationships with Project Outcomes &mdash; Poster
<b>
(SIGCSE '19)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>, R. S. Mansur, S. H. Edwards, C. A. Shaffer
<br>
<a href='assets/documents/sigcse2019-debugging-poster.pdf'>pdf</a>
<a href='https://doi.org/10.1145/3287324.3293794'>link</a>
</div>
</summary>
<p>
<b>Abstract:</b>
Debugging is an important part of the software development process, studied by both the CS education and software engineering communities. Most prior work has focused either on novice or professional programmers. Intermediate-to-advanced students (such as those enrolled in post-CS2 Data Structures courses) who are working on large and complex projects have largely been ignored. We present results from an empirical observational study that examined junior-level undergraduate students' debugging practices on relatively large (4-week lifecycle) projects, using IDE clickstream data collected by a custom Eclipse plugin. Specifically, we hypothesize that there are differing debugging behaviors exhibited, and that differing behaviors lead to differing project out-comes. For example, how often do students use the symbolic debugger available in modern IDEs, versus how often do they use diagnostic print statements, or both? What triggers a debugging session? What follows a debugging session? Does it matter when in the project lifecycle that debugging takes place? We have a number of interesting preliminary results. When using the debugger, there was a negative relationship between step-over and step-into actions versus final course grades, indicating that when students "spin their wheels" while debugging, they tend to perform more poorly. Students also tend to perform better on the project when debugging takes place earlier in the overall project life-cycle. We developed an algorithm to identify diagnostic print statements in the students' projects. We found that over 90% used at least one diagnostic print statement, and about 75% used the symbolic debugger, at least once in any given project.
</p>
</details>
<details class='artifact'>
<summary>
Toward Continuous Assessment of the Programming Process &mdash; Student Research Competition
<b>
(SIGCSE '18)
</b>
<div class='sub'>
<b>A. M. Kazerouni</b>
<br>
<a href='assets/documents/sigcse-src-2018-poster.pdf'>pdf</a>
<a href='assets/slides/sigcse2018-src-slides.pdf'>slides</a>
<a href='https://doi.org/10.1145/3159450.3162337'>link</a>
<div class='award'>
1st Place
</div>
</div>
</summary>
<p>
<b>Abstract:</b>
Assessment of software tends to focus on postmortem evaluation of metrics like correctness, mergeability, and code coverage. This is evidenced in the current practices of continuous integration and deployment that focus on software's ability to pass unit tests before it can be merged into a deployment pipeline. However, little attention or tooling is given to the assessment of the software development process itself. Good process becomes both more challenging and more critical as software complexity increases. Real-time evaluation and feedback about a software developer's skills, such as incremental development, testing, and time management, could greatly increase productivity and improve the ability to write tested andcorrect code. My work focuses on the collection and analysis of fine-grained programming process data to help quantitatively model the programming process in terms of these metrics. I report on my research problem, presenting past work involving the collection and analysis of IDE event data from junior level students working on large and complex projects. The goal is to quantify the programming process in terms of incremental development and procrastination. I also present a long-term vision for my research and present work planned in the short term as a step toward that vision.
</p>
</details>
</div>
</div>
</div>


		</article>
		<footer>
  <div class='get-in-touch'>Get in touch.<br></div>
  <div class='address'><a href="mailto:ayaan@vt.edu">ayaan@vt.edu</a></div>
  <div class='address'>2000 Torgersen Hall, 620 Drillfield Drive</div>
  <div class='contact'>
    <a href='https://twitter.com/ayaankazerouni'>
      <i class='fab fa-twitter-square' aria-label="Twitter" title="Twitter"></i>
      <span class='visible-hidden'>Twitter profile</span>
    </a>
    <a href='https://github.com/ayaankazerouni'>
      <i class='fab fa-github' aria-label="GitHub", title="GitHub"></i>
      <span class='visible-hidden'>GitHub profile</span>
    </a>
    <!-- <a href='https://www.linkedin.com/in/ayaankazerouni'>
      <i class='fab fa-linkedin' aria-label="LinkedIn" title="LinkedIn"></i>
      <span class='visible-hidden'>LinkedIn profile</span>
    </a> -->
    <a href='https://scholar.google.com/citations?user=jGSnCrUAAAAJ&hl=en'>
      <i class='ai ai-google-scholar-square' aria-label="Google Scholar" title="Google Scholar"></i>
      <span class='visible-hidden'>Google Scholar profile</span>
    </a>
    <a href='https://orcid.org/0000-0002-6574-1278'>
      <i class='ai ai-orcid-square' aria-label="OrcID" title="OrcID"></i>
      <span class='visible-hidden'>OrcID profile</span>
    </a>
  </div>
</footer>

	</main>
</body>

</html>
